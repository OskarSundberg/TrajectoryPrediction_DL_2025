{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import gc\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch Dataset class for Transformer models.\n",
    "    This dataset provides pre-processed source and target sequences for training.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data):\n",
    "        \"\"\"\n",
    "        Initialize the dataset.\n",
    "\n",
    "        Args:\n",
    "            data (dict): A dictionary containing:\n",
    "                - 'src': Source sequence tensor (shape: [num_samples, src_length, num_features]).\n",
    "                - 'tgt': Target sequence tensor (shape: [num_samples, tgt_length, num_features]).\n",
    "        \"\"\"\n",
    "        self.data = data  # Store the preprocessed source and target sequences.\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the number of samples in the dataset.\n",
    "        \"\"\"\n",
    "        return self.data[\"src\"].shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieve a single sample from the dataset.\n",
    "\n",
    "        Args:\n",
    "            idx (int): The index of the sample to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            data (dict): A dictionary containing:\n",
    "                - 'src': Source sequence tensor (shape: [src_length, num_features]).\n",
    "                - 'tgt': Target sequence tensor (shape: [tgt_length, num_features]).\n",
    "        \"\"\"\n",
    "        src, tgt = self.data['src'], self.data['tgt']\n",
    "        data = {\n",
    "            \"src\": src[idx, :, :].type(torch.float16),  # Source sequence as half-precision tensor.\n",
    "            \"tgt\": tgt[idx, :, :].type(torch.float16)   # Target sequence as half-precision tensor.\n",
    "        }\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class STARDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch Dataset class for training with preprocessed trajectory and distance data.\n",
    "    \"\"\"\n",
    "    def __init__(self, data, max_num_agents):\n",
    "        self.data=data\n",
    "        self.max_num_agents = max_num_agents\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Initialize the dataset.\n",
    "\n",
    "        Args:\n",
    "            data (dict): A dictionary containing:\n",
    "                - 'src': Source trajectory tensor (shape: [num_samples, src_length, num_features]).\n",
    "                - 'tgt': Target trajectory tensor (shape: [num_samples, tgt_length, num_features]).\n",
    "                - 'distance': Distances to agents and objects (shape: [num_samples, src_length, num_distances]).\n",
    "                - 'type': Types of agents and objects (shape: [num_samples, src_length, num_types]).\n",
    "        \"\"\"\n",
    "        return self.data[\"src\"].shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieve a single sample from the dataset.\n",
    "\n",
    "        Args:\n",
    "            idx (int): The index of the sample to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            data (dict): A dictionary containing:\n",
    "                - 'src': Source trajectory tensor.\n",
    "                - 'tgt': Target trajectory tensor.\n",
    "                - 'distance': Distance tensor.\n",
    "                - 'type': Type tensor.\n",
    "        \"\"\"\n",
    "        src, tgt, distance, type = self.data['src'], self.data['tgt'], self.data['distance'], self.data['type']\n",
    "        data = {\n",
    "          \"src\": src[idx, :, :].type(torch.float32),\n",
    "          \"tgt\": tgt[idx, :, :].type(torch.float32),\n",
    "          \"distance\": distance[idx, :, :self.max_num_agents].type(torch.float32),\n",
    "          \"type\": type[idx, :, :self.max_num_agents].type(torch.long)\n",
    "        }\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAESTARDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch Dataset class for training with preprocessed trajectory and distance data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data : dict):\n",
    "        \"\"\"\n",
    "        Initialize the dataset.\n",
    "\n",
    "        Args:\n",
    "            data (dict): A dictionary containing:\n",
    "                - 'src': Source trajectory tensor (shape: [num_samples, src_length, num_features]).\n",
    "                - 'tgt': Target trajectory tensor (shape: [num_samples, tgt_length, num_features]).\n",
    "                - 'distance': Distances to agents and objects (shape: [num_samples, src_length, num_distances]).\n",
    "                - 'type': Types of agents and objects (shape: [num_samples, src_length, num_types]).\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the number of samples in the dataset.\n",
    "        \"\"\"\n",
    "        return self.data[\"src\"].shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieve a single sample from the dataset.\n",
    "\n",
    "        Args:\n",
    "            idx (int): The index of the sample to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            data (dict): A dictionary containing:\n",
    "                - 'src': Source trajectory tensor.\n",
    "                - 'tgt': Target trajectory tensor.\n",
    "                - 'distance': Distance tensor.\n",
    "                - 'type': Type tensor.\n",
    "        \"\"\"\n",
    "        src, tgt, distance, dist_type = self.data['src'], self.data['tgt'], self.data['distance'], self.data['type']\n",
    "        data = {\n",
    "            \"src\": src[idx, :, :].type(torch.float32),\n",
    "            \"tgt\": tgt[idx, :, :].type(torch.float32),\n",
    "            \"distance\": distance[idx, :, :].type(torch.float32),\n",
    "            \"type\": dist_type[idx, :, :].type(torch.long)\n",
    "        }\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBaseEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    A PyTorch module for creating input embeddings for a transformer model.\n",
    "    \n",
    "    This class embeds input sequences using linear projections for specific features and \n",
    "    adds positional encoding to the embeddings. It supports separate embeddings for \n",
    "    source (`src`) and target (`tgt`) sequences.\n",
    "\n",
    "    Attributes:\n",
    "        embedding_dims (list[int]): List of embedding dimensions for each feature.\n",
    "        size (int): Total embedding size (sum of `embedding_dims`).\n",
    "        linear_layers_tgt (nn.ModuleList): Linear layers for projecting the first two features of `tgt` inputs.\n",
    "        linear_layers_src (nn.ModuleList): Linear layers for projecting the first two features of `src` inputs.\n",
    "        positional_encoding_src (torch.Tensor): Positional encoding tensor for `src` inputs.\n",
    "        positional_encoding_tgt (torch.Tensor): Positional encoding tensor for `tgt` inputs.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_dims, sequence_length_src=10, sequence_length_tgt=40):\n",
    "        \"\"\"\n",
    "        Initializes the embedding class with specified dimensions and sequence lengths.\n",
    "\n",
    "        Args:\n",
    "            embedding_dims (list[int]): Embedding dimensions for the features.\n",
    "            sequence_length_src (int, optional): Length of the source sequence. Defaults to 10.\n",
    "            sequence_length_tgt (int, optional): Length of the target sequence. Defaults to 40.\n",
    "        \"\"\"\n",
    "        super(TransformerBaseEmbedding, self).__init__()\n",
    "        self.embedding_dims = embedding_dims\n",
    "        self.size = sum(embedding_dims)\n",
    "\n",
    "        # Linear layers for projecting the first two features of `tgt`\n",
    "        self.linear_layers_tgt = nn.ModuleList([\n",
    "            nn.Linear(1, embedding_dim)\n",
    "            for embedding_dim in embedding_dims[:2]\n",
    "        ])\n",
    "\n",
    "        # Linear layers for projecting the first two features of `src`\n",
    "        self.linear_layers_src = nn.ModuleList([\n",
    "            nn.Linear(1, embedding_dim)\n",
    "            for embedding_dim in embedding_dims[:2]\n",
    "        ])\n",
    "\n",
    "        # Positional encodings for `src` and `tgt` sequences\n",
    "        self.positional_encoding_src = self._generate_positional_encoding(self.size, sequence_length_src).cuda()\n",
    "        self.positional_encoding_tgt = self._generate_positional_encoding(self.size, sequence_length_tgt).cuda()\n",
    "\n",
    "    def _generate_positional_encoding(self, embedding_size, sequence_length):\n",
    "        \"\"\"\n",
    "        Generates positional encoding for a sequence of a given length and embedding size.\n",
    "\n",
    "        Args:\n",
    "            embedding_size (int): Dimensionality of the embedding space.\n",
    "            sequence_length (int): Length of the sequence.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: A tensor containing positional encodings with shape (1, sequence_length, embedding_size).\n",
    "        \"\"\"\n",
    "        positional_encoding = torch.zeros(sequence_length, embedding_size)\n",
    "        position = torch.arange(0, sequence_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embedding_size, 2).float() * (-torch.log(torch.tensor(10000.0)) / embedding_size))\n",
    "\n",
    "        # Apply sine and cosine to alternate dimensions\n",
    "        positional_encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        positional_encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        return positional_encoding.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    def forward(self, input_tensor, is_src=True):\n",
    "        \"\"\"\n",
    "        Forward pass for embedding input sequences with optional source or target embeddings.\n",
    "\n",
    "        Args:\n",
    "            input_tensor (torch.Tensor): Input tensor of shape (batch_size, seq_length, num_features).\n",
    "            is_src (bool, optional): If `True`, processes as source (`src`) sequence; otherwise as target (`tgt`).\n",
    "                                     Defaults to `True`.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Embedded tensor of shape (batch_size, seq_length, total_embedding_size).\n",
    "        \"\"\"\n",
    "        batch_size, seq_length, num_features = input_tensor.size()\n",
    "\n",
    "        # Initialize an empty tensor for the embedded output\n",
    "        embedded_tensor = torch.zeros(batch_size, seq_length, self.size).cuda()\n",
    "\n",
    "        # Process source (`src`) sequences\n",
    "        if is_src:\n",
    "            start_index = 0\n",
    "            for i, linear_layer in enumerate(self.linear_layers_src):\n",
    "                # Apply the linear layer to the i-th feature\n",
    "                linear_output = linear_layer(input_tensor[:, :, i].view(-1, 1)).cuda()\n",
    "\n",
    "                # Determine the embedding dimension for the current feature\n",
    "                embedding_dim = self.embedding_dims[i]\n",
    "                end_index = start_index + embedding_dim\n",
    "\n",
    "                # Place the linearly projected feature into the correct slice of the output tensor\n",
    "                embedded_tensor[:, :, start_index:end_index] = linear_output.view(batch_size, seq_length, embedding_dim).cuda()\n",
    "                start_index = end_index\n",
    "\n",
    "        # Process target (`tgt`) sequences\n",
    "        else:\n",
    "            start_index = 0\n",
    "            for i, linear_layer in enumerate(self.linear_layers_tgt):\n",
    "                # Apply the linear layer to the i-th feature\n",
    "                linear_output = linear_layer(input_tensor[:, :, i].view(-1, 1)).cuda()\n",
    "\n",
    "                # Determine the embedding dimension for the current feature\n",
    "                embedding_dim = self.embedding_dims[i]\n",
    "                end_index = start_index + embedding_dim\n",
    "\n",
    "                # Place the linearly projected feature into the correct slice of the output tensor\n",
    "                embedded_tensor[:, :, start_index:end_index] = linear_output.view(batch_size, seq_length, embedding_dim).cuda()\n",
    "                start_index = end_index\n",
    "\n",
    "        # Add positional encoding to the embeddings\n",
    "        positional_encoding = self.positional_encoding_src if is_src else self.positional_encoding_tgt\n",
    "        positional_encoding = positional_encoding.cuda()\n",
    "        embedded_tensor = embedded_tensor + positional_encoding\n",
    "\n",
    "        # Return the final embedded tensor\n",
    "        return embedded_tensor.view(batch_size, seq_length, -1).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    A PyTorch module for embedding input sequences with source (`src`) and target (`tgt`) features \n",
    "    using linear projections for numerical features and embedding layers for categorical features. \n",
    "    Additionally, positional encodings are added to the embeddings.\n",
    "    \n",
    "    Attributes:\n",
    "        size (int): Total size of embeddings (sum of src_dims).\n",
    "        src_dims (list[int]): List of embedding dimensions for the source sequence.\n",
    "        tgt_dims (list[int]): List of embedding dimensions for the target sequence.\n",
    "        src_len (int): Length of the source sequence.\n",
    "        tgt_len (int): Length of the target sequence.\n",
    "        linear_layers_src (nn.ModuleList): List of linear layers for the source sequence features.\n",
    "        linear_layers_tgt (nn.ModuleList): List of linear layers for the target sequence features.\n",
    "        embedding_layer_src (nn.Embedding): Embedding layer for the last categorical feature of the source sequence.\n",
    "        positional_encoding_src (torch.Tensor): Positional encoding for the source sequence.\n",
    "        positional_encoding_tgt (torch.Tensor): Positional encoding for the target sequence.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, src_dims, tgt_dims, num_agents, sequence_length_src=10, sequence_length_tgt=40):\n",
    "        \"\"\"\n",
    "        Initializes the embedding class with specified dimensions and sequence lengths for source \n",
    "        and target sequences.\n",
    "        \n",
    "        Args:\n",
    "            src_dims (list[int]): List of embedding dimensions for the features in the source sequence.\n",
    "            tgt_dims (list[int]): List of embedding dimensions for the features in the target sequence.\n",
    "            num_agents (int): Number of distinct categories for the last feature in the source sequence.\n",
    "            sequence_length_src (int, optional): Length of the source sequence. Defaults to 10.\n",
    "            sequence_length_tgt (int, optional): Length of the target sequence. Defaults to 40.\n",
    "        \"\"\"\n",
    "        super(TransformerEmbedding, self).__init__()\n",
    "        \n",
    "        self.size = sum(src_dims)  # Total embedding size (sum of src_dims)\n",
    "        self.src_dims = src_dims\n",
    "        self.tgt_dims = tgt_dims\n",
    "        self.src_len = sequence_length_src\n",
    "        self.tgt_len = sequence_length_tgt \n",
    "        \n",
    "        # Linear layers for projecting the first 3 features of the source sequence\n",
    "        self.linear_layers_src = nn.ModuleList([\n",
    "            nn.Linear(1, embedding_dim)\n",
    "            for embedding_dim in src_dims[:3]  # Use linear layers for the first 3 features of src\n",
    "        ])\n",
    "        \n",
    "        # Linear layers for projecting the first 2 features of the target sequence\n",
    "        self.linear_layers_tgt = nn.ModuleList([\n",
    "            nn.Linear(1, embedding_dim)\n",
    "            for embedding_dim in tgt_dims[:2]  # Use linear layers for the first 2 features of tgt\n",
    "        ])\n",
    "\n",
    "        # Embedding layer for the last feature of the source sequence (categorical)\n",
    "        self.embedding_layer_src = nn.Embedding(num_agents, src_dims[-1]).cuda()\n",
    "\n",
    "        # Generate positional encodings for both source and target sequences\n",
    "        self.positional_encoding_src = self._generate_positional_encoding(self.size, sequence_length_src).cuda()\n",
    "        self.positional_encoding_tgt = self._generate_positional_encoding(self.size, sequence_length_tgt).cuda()\n",
    "\n",
    "    def _generate_positional_encoding(self, embedding_size, sequence_length):\n",
    "        \"\"\"\n",
    "        Generates a sinusoidal positional encoding for a given sequence length and embedding size.\n",
    "        \n",
    "        Args:\n",
    "            embedding_size (int): Dimensionality of the embedding space.\n",
    "            sequence_length (int): Length of the sequence for which to generate the encoding.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Positional encoding tensor with shape (1, sequence_length, embedding_size).\n",
    "        \"\"\"\n",
    "        positional_encoding = torch.zeros(sequence_length, embedding_size)  # Initialize the encoding tensor\n",
    "        position = torch.arange(0, sequence_length, dtype=torch.float).unsqueeze(1)  # Position tensor\n",
    "        div_term = torch.exp(torch.arange(0, embedding_size, 2).float() * (-torch.log(torch.tensor(10000.0)) / embedding_size))  # Scaling factor\n",
    "        \n",
    "        # Apply sine and cosine functions to alternating dimensions of the encoding\n",
    "        positional_encoding[:, 0::2] = torch.sin(position * div_term)  # Even indices\n",
    "        positional_encoding[:, 1::2] = torch.cos(position * div_term)  # Odd indices\n",
    "        \n",
    "        return positional_encoding.unsqueeze(0)  # Add batch dimension and return\n",
    "\n",
    "    def forward(self, input_tensor, is_src=True):\n",
    "        \"\"\"\n",
    "        Forward pass for embedding input sequences, with separate handling for source and target sequences.\n",
    "        \n",
    "        Args:\n",
    "            input_tensor (torch.Tensor): Input tensor of shape (batch_size, seq_length, num_features).\n",
    "            is_src (bool, optional): If `True`, processes the source sequence; otherwise processes the target sequence. Defaults to `True`.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The final embedded tensor of shape (batch_size, seq_length, total_embedding_size).\n",
    "        \"\"\"\n",
    "        batch_size, seq_length, num_features = input_tensor.size()\n",
    "        \n",
    "        # Select the embedding dimensions based on whether the sequence is source or target\n",
    "        embedding_dims = self.src_dims if is_src else self.tgt_dims\n",
    "\n",
    "        # Initialize an empty tensor for the embedded output\n",
    "        embedded_tensor = torch.zeros(batch_size, seq_length, self.size).cuda()\n",
    "\n",
    "        # Handle source sequence embedding\n",
    "        if is_src:\n",
    "            start_index = 0\n",
    "            for i, linear_layer in enumerate(self.linear_layers_src):\n",
    "                # Linearly project each feature of the source sequence\n",
    "                linear_output = linear_layer(input_tensor[:, :, i].view(-1, 1)).cuda()\n",
    "\n",
    "                # Get the embedding dimension for the current feature\n",
    "                embedding_dim = embedding_dims[i]\n",
    "                end_index = start_index + embedding_dim\n",
    "\n",
    "                # Place the linearly projected feature at the correct position in the embedded tensor\n",
    "                embedded_tensor[:, :, start_index:end_index] = linear_output.view(batch_size, seq_length, embedding_dim).cuda()\n",
    "                start_index = end_index\n",
    "\n",
    "            # Embed the last feature of the source sequence (categorical)\n",
    "            embedded_tensor[:, :, start_index:] = self.embedding_layer_src(input_tensor[:, :, 3].long()).cuda()\n",
    "\n",
    "        # Handle target sequence embedding\n",
    "        else:\n",
    "            start_index = 0\n",
    "            for i, linear_layer in enumerate(self.linear_layers_tgt):\n",
    "                # Linearly project each feature of the target sequence\n",
    "                linear_output = linear_layer(input_tensor[:, :, i].view(-1, 1)).cuda()\n",
    "\n",
    "                # Get the embedding dimension for the current feature\n",
    "                embedding_dim = embedding_dims[i]\n",
    "                end_index = start_index + embedding_dim\n",
    "\n",
    "                # Place the linearly projected feature at the correct position in the embedded tensor\n",
    "                embedded_tensor[:, :, start_index:end_index] = linear_output.view(batch_size, seq_length, embedding_dim).cuda()\n",
    "                start_index = end_index\n",
    "\n",
    "        # Add positional encoding to the embeddings\n",
    "        positional_encoding = self.positional_encoding_src if is_src else self.positional_encoding_tgt\n",
    "        embedded_tensor = embedded_tensor + positional_encoding.cuda()\n",
    "\n",
    "        # Reshape and return the final embedded tensor\n",
    "        embedded_tensor = embedded_tensor.view(batch_size, seq_length, -1).cuda()\n",
    "        return embedded_tensor.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class STAREmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    A PyTorch module for embedding input sequences with source (`src`) and distance-related features.\n",
    "\n",
    "    This class embeds source and distance sequences using linear projections for numerical features,\n",
    "    embedding layers for categorical features, and applies positional encoding to the resulting embeddings.\n",
    "    \n",
    "    Attributes:\n",
    "        num_features (int): Number of features in the source (`src`) sequence.\n",
    "        distance_len (int): Number of distance-related features.\n",
    "        type_len (int): Number of categorical features in the distance sequence.\n",
    "        src_dims (list[int]): Embedding dimensions for the source sequence.\n",
    "        dist_dims (list[int]): Embedding dimensions for the distance-related sequence.\n",
    "        type_dims (list[int]): Embedding dimensions for categorical features in the distance sequence.\n",
    "        size (int): Total embedding size (sum of source feature dimensions).\n",
    "        linear_layers_src (nn.ModuleList): Linear layers for embedding source sequence features.\n",
    "        linear_layers_distance (nn.ModuleList): Linear layers for embedding distance-related features.\n",
    "        type_layers_distance (nn.ModuleList): Embedding layers for categorical features in the distance sequence.\n",
    "        embedding_layer_src (nn.Embedding): Embedding layer for the last categorical feature of the source sequence.\n",
    "        positional_encoding_src (torch.Tensor): Positional encoding for the source sequence.\n",
    "        positional_encoding_distance (torch.Tensor): Positional encoding for the distance sequence.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, src_dims, dist_dims, type_dims, num_types, sequence_length=10):\n",
    "        \"\"\"\n",
    "        Initializes the embedding class with specified dimensions and sequence length.\n",
    "\n",
    "        Args:\n",
    "            src_dims (list[int]): List of embedding dimensions for the features in the source sequence.\n",
    "            dist_dims (list[int]): List of embedding dimensions for the features in the distance sequence.\n",
    "            type_dims (list[int]): List of embedding dimensions for categorical features.\n",
    "            num_types (int): Number of distinct categories for the last feature of the source sequence.\n",
    "            sequence_length (int, optional): Length of the source and distance sequences. Defaults to 10.\n",
    "        \"\"\"\n",
    "        super(STAREmbedding, self).__init__()\n",
    "        \n",
    "        # Initialize attributes for feature counts and dimensions\n",
    "        self.num_features = len(src_dims)\n",
    "        self.distance_len = len(dist_dims)\n",
    "        self.type_len = len(type_dims)\n",
    "        self.size = sum(src_dims)\n",
    "        \n",
    "        self.src_dims = src_dims\n",
    "        self.dist_dims = dist_dims\n",
    "        self.type_dims = type_dims\n",
    "\n",
    "        # Linear layers for projecting source sequence features\n",
    "        self.linear_layers_src = nn.ModuleList([\n",
    "            nn.Linear(1, embedding_dim)\n",
    "            for embedding_dim in src_dims[:self.num_features - 1]  # Exclude the last feature for different treatment\n",
    "        ])\n",
    "        \n",
    "        # Linear layers for projecting distance-related features\n",
    "        self.linear_layers_distance = nn.ModuleList([\n",
    "            nn.Linear(1, embedding_dim)\n",
    "            for embedding_dim in dist_dims[:]\n",
    "        ])\n",
    "        \n",
    "        # Embedding layers for categorical features in the distance sequence\n",
    "        self.type_layers_distance = nn.ModuleList([\n",
    "            nn.Embedding(num_types, embedding_dim).cuda()\n",
    "            for embedding_dim in type_dims[:]\n",
    "        ])\n",
    "        \n",
    "        # Embedding layer for the last categorical feature of the source sequence\n",
    "        self.embedding_layer_src = nn.Embedding(num_types, src_dims[-1]).cuda()\n",
    "\n",
    "        # Generate positional encodings for source and distance sequences\n",
    "        self.positional_encoding_src = self._generate_positional_encoding(self.size, sequence_length).cuda()\n",
    "        self.positional_encoding_distance = self._generate_positional_encoding(self.size, sequence_length).cuda()\n",
    "\n",
    "    def _generate_positional_encoding(self, embedding_size, sequence_length):\n",
    "        \"\"\"\n",
    "        Generates a sinusoidal positional encoding for a given sequence length and embedding size.\n",
    "        \n",
    "        Args:\n",
    "            embedding_size (int): Dimensionality of the embedding space.\n",
    "            sequence_length (int): Length of the sequence for which to generate the encoding.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Positional encoding tensor with shape (1, sequence_length, embedding_size).\n",
    "        \"\"\"\n",
    "        positional_encoding = torch.zeros(sequence_length, embedding_size).cuda()\n",
    "        position = torch.arange(0, sequence_length, dtype=torch.float).unsqueeze(1).cuda()\n",
    "        div_term = torch.exp(torch.arange(0, embedding_size, 2).float().cuda() * (-torch.log(torch.tensor(10000.0)) / embedding_size)).cuda()\n",
    "        \n",
    "        # Apply sine and cosine functions to alternate dimensions of the encoding\n",
    "        positional_encoding[:, 0::2] = torch.sin(position * div_term).cuda()\n",
    "        positional_encoding[:, 1::2] = torch.cos(position * div_term).cuda()\n",
    "        \n",
    "        return positional_encoding.unsqueeze(0).cuda()  # Add batch dimension\n",
    "\n",
    "    def forward(self, input_tensor, is_src=True, src_tensor=None, type_tensor=None):\n",
    "        \"\"\"\n",
    "        Forward pass for embedding input sequences, with separate handling for source and distance sequences.\n",
    "        \n",
    "        Args:\n",
    "            input_tensor (torch.Tensor): Input tensor of shape (batch_size, seq_length, num_features).\n",
    "            is_src (bool, optional): If `True`, processes the source sequence; otherwise processes the distance sequence. Defaults to `True`.\n",
    "            src_tensor (torch.Tensor, optional): Tensor containing source sequence features. Defaults to `None`.\n",
    "            type_tensor (torch.Tensor, optional): Tensor containing categorical features for the distance sequence. Defaults to `None`.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The final embedded tensor of shape (batch_size, seq_length, total_embedding_size).\n",
    "        \"\"\"\n",
    "        batch_size, seq_length, num_features = input_tensor.size()\n",
    "        \n",
    "        # Select the embedding dimensions based on whether the sequence is source or distance\n",
    "        embedding_dims = self.src_dims if is_src else self.dist_dims\n",
    "\n",
    "        # Initialize an empty tensor for the embedded output\n",
    "        embedded_tensor = torch.zeros(batch_size, seq_length, self.size).cuda()\n",
    "\n",
    "        # Handle source sequence embedding\n",
    "        if is_src:\n",
    "            start_index = 0\n",
    "            for i, linear_layer in enumerate(self.linear_layers_src):\n",
    "                # Linearly project each feature of the source sequence\n",
    "                linear_output = linear_layer(input_tensor[:, :, i].view(-1, 1)).cuda()\n",
    "\n",
    "                # Get the embedding dimension for the current feature\n",
    "                embedding_dim = embedding_dims[i]\n",
    "                end_index = start_index + embedding_dim\n",
    "\n",
    "                # Place the linearly projected feature at the correct position in the embedded tensor\n",
    "                embedded_tensor[:, :, start_index:end_index] = linear_output.view(batch_size, seq_length, embedding_dim).cuda()\n",
    "                start_index = end_index\n",
    "\n",
    "            # Embed the last feature of the source sequence (categorical)\n",
    "            embedded_tensor[:, :, start_index:] = self.embedding_layer_src(input_tensor[:, :, 3].long()).cuda()\n",
    "\n",
    "        # Handle distance sequence embedding\n",
    "        else:\n",
    "            embedded_tensor = torch.zeros(batch_size, seq_length, self.size).cuda()\n",
    "            start_index = 0\n",
    "            for i, (linear_layer, embedding_layer) in enumerate(zip(self.linear_layers_distance, self.type_layers_distance)):\n",
    "                # Linearly project each feature of the distance sequence\n",
    "                linear_output = linear_layer(input_tensor[:, :, i].view(-1, 1)).cuda()\n",
    "\n",
    "                # Get the embedding dimension for the current feature\n",
    "                embedding_dim = embedding_dims[i]\n",
    "                \n",
    "                # Embed the categorical features in the distance sequence\n",
    "                embedding_output = embedding_layer(type_tensor[:, :, i].long()).cuda()\n",
    "                embedded_tensor[:, :, start_index:start_index + embedding_output.shape[-1]] = embedding_output\n",
    "                start_index = start_index + embedding_output.shape[-1]\n",
    "\n",
    "                # Place the linearly projected distance feature into the tensor\n",
    "                end_index = start_index + embedding_dim\n",
    "                embedded_tensor[:, :, start_index:end_index] = linear_output.view(batch_size, seq_length, embedding_dim).cuda()\n",
    "                start_index = end_index\n",
    "\n",
    "        # Add positional encoding to the embeddings\n",
    "        positional_encoding = self.positional_encoding_src if is_src else self.positional_encoding_distance\n",
    "        embedded_tensor = embedded_tensor + positional_encoding\n",
    "\n",
    "        # Reshape the output tensor and return the final result\n",
    "        embedded_tensor = embedded_tensor.view(batch_size, seq_length, -1).cuda()\n",
    "        return embedded_tensor.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAESTAREmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    A PyTorch module for embedding input sequences with source and distance-related features.\n",
    "    \n",
    "    This class embeds the source (`src`) and distance-related (`dist`) sequences using \n",
    "    separate linear projections for the numerical features, embedding layers for categorical \n",
    "    features, and adds positional encoding for both types of inputs.\n",
    "\n",
    "    Attributes:\n",
    "        num_features (int): Number of features in the source (`src`) input.\n",
    "        dist_len (int): Number of distance-related features.\n",
    "        type_len (int): Number of types in the categorical input.\n",
    "        size (int): Total embedding size (sum of source feature dimensions).\n",
    "        src_dims (list[int]): Embedding dimensions for the source sequence.\n",
    "        dist_dims (list[int]): Embedding dimensions for the distance sequence.\n",
    "        linear_layers_src (nn.ModuleList): Linear layers for embedding source features.\n",
    "        linear_layers_distance (nn.ModuleList): Linear layers for embedding distance features.\n",
    "        type_layers_distance (nn.ModuleList): Embedding layers for categorical features in the distance sequence.\n",
    "        embedding_layer_src (nn.Embedding): Embedding layer for the last categorical feature of source sequence.\n",
    "        positional_encoding_src (torch.Tensor): Positional encoding for the source sequence.\n",
    "        positional_encoding_distance (torch.Tensor): Positional encoding for the distance sequence.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, src_dims, dist_dims, type_dims, num_types, num_types_dist, sequence_length_src=10):\n",
    "        \"\"\"\n",
    "        Initializes the embedding class with specified dimensions and sequence lengths.\n",
    "        \n",
    "        Args:\n",
    "            src_dims (list[int]): List of embedding dimensions for the features in the source sequence.\n",
    "            dist_dims (list[int]): List of embedding dimensions for the features in the distance sequence.\n",
    "            type_dims (list[int]): List of embedding dimensions for categorical features.\n",
    "            num_types (int): Number of distinct categories for the last feature of the source sequence.\n",
    "            num_types_dist (int): Number of distinct categories for the categorical features in the distance sequence.\n",
    "            sequence_length_src (int, optional): Length of the source sequence. Defaults to 10.\n",
    "        \"\"\"\n",
    "        super(SAESTAREmbedding, self).__init__()\n",
    "        self.num_features = len(src_dims)\n",
    "        self.dist_len = len(dist_dims)\n",
    "        self.type_len = len(type_dims)\n",
    "        self.size = sum(src_dims)\n",
    "        self.src_dims = src_dims\n",
    "        self.dist_dims = dist_dims\n",
    "\n",
    "        # Linear layers for projecting the first features of the source sequence\n",
    "        self.linear_layers_src = nn.ModuleList([\n",
    "            nn.Linear(1, embedding_dim)\n",
    "            for embedding_dim in src_dims[:self.num_features - 1]  # Exclude the last feature for different treatment\n",
    "        ])\n",
    "\n",
    "        # Linear layers for projecting distance-related features\n",
    "        self.linear_layers_distance = nn.ModuleList([\n",
    "            nn.Linear(1, embedding_dim)\n",
    "            for embedding_dim in dist_dims[:]\n",
    "        ])\n",
    "        \n",
    "        # Embedding layers for categorical features in the distance sequence\n",
    "        self.type_layers_distance = nn.ModuleList([\n",
    "            nn.Embedding(num_types_dist + num_types , embedding_dim).cuda()\n",
    "            for embedding_dim in type_dims[:]\n",
    "        ])\n",
    "\n",
    "        # Embedding layer for the last categorical feature of the source sequence\n",
    "        self.embedding_layer_src = nn.Embedding(num_types, src_dims[-1]).cuda()\n",
    "\n",
    "        # Generate positional encoding for source and distance sequences\n",
    "        self.positional_encoding_src = self._generate_positional_encoding(self.size, sequence_length_src).cuda()\n",
    "        self.positional_encoding_distance = self._generate_positional_encoding(self.size, sequence_length_src).cuda()\n",
    "\n",
    "    def _generate_positional_encoding(self, embedding_size, sequence_length):\n",
    "        \"\"\"\n",
    "        Generates a sinusoidal positional encoding for a given sequence length and embedding size.\n",
    "\n",
    "        Args:\n",
    "            embedding_size (int): The dimensionality of the embedding space.\n",
    "            sequence_length (int): The length of the sequence for which to generate the encoding.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: A tensor containing the positional encoding for the sequence.\n",
    "        \"\"\"\n",
    "        positional_encoding = torch.zeros(sequence_length, embedding_size).cuda()\n",
    "        position = torch.arange(0, sequence_length, dtype=torch.float).unsqueeze(1).cuda()\n",
    "        div_term = torch.exp(torch.arange(0, embedding_size, 2).float().cuda() * (-torch.log(torch.tensor(10000.0)) / embedding_size)).cuda()\n",
    "        \n",
    "        # Apply sine and cosine functions to alternate dimensions of the encoding\n",
    "        positional_encoding[:, 0::2] = torch.sin(position * div_term).cuda()\n",
    "        positional_encoding[:, 1::2] = torch.cos(position * div_term).cuda()\n",
    "\n",
    "        return positional_encoding.unsqueeze(0).cuda()  # Add batch dimension\n",
    "\n",
    "    def forward(self, input_tensor, is_src=True, src_tensor=None, type_tensor=None):\n",
    "        \"\"\"\n",
    "        Forward pass for embedding input sequences, with separate handling for source and distance sequences.\n",
    "\n",
    "        Args:\n",
    "            input_tensor (torch.Tensor): Input tensor of shape (batch_size, seq_length, num_features).\n",
    "            is_src (bool, optional): If `True`, processes the source sequence; otherwise processes the distance sequence. Defaults to `True`.\n",
    "            src_tensor (torch.Tensor, optional): Tensor containing source sequence features. Defaults to `None`.\n",
    "            type_tensor (torch.Tensor, optional): Tensor containing categorical features for the distance sequence. Defaults to `None`.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The final embedded tensor of shape (batch_size, seq_length, total_embedding_size).\n",
    "        \"\"\"\n",
    "        batch_size, seq_length, num_features = input_tensor.size()\n",
    "\n",
    "        # Select the embedding dimensions based on whether the sequence is source or distance\n",
    "        embedding_dims = self.src_dims if is_src else self.dist_dims\n",
    "\n",
    "        # Initialize an empty tensor for the embedded output\n",
    "        embedded_tensor = torch.zeros(batch_size, seq_length, self.size).cuda()\n",
    "\n",
    "        # Handle source sequence embedding\n",
    "        if is_src:\n",
    "            start_index = 0\n",
    "            for i, linear_layer in enumerate(self.linear_layers_src):\n",
    "                # Linearly project each feature of the source sequence\n",
    "                linear_output = linear_layer(input_tensor[:, :, i].view(-1, 1)).cuda()\n",
    "\n",
    "                # Get the embedding dimension for the current feature\n",
    "                embedding_dim = embedding_dims[i]\n",
    "                end_index = start_index + embedding_dim\n",
    "\n",
    "                # Place the linearly projected feature at the correct position in the embedded tensor\n",
    "                embedded_tensor[:, :, start_index:end_index] = linear_output.view(batch_size, seq_length, embedding_dim).cuda()\n",
    "                start_index = end_index\n",
    "\n",
    "            # Embed the last feature of the source sequence (categorical)\n",
    "            embedded_tensor[:, :, start_index:] = self.embedding_layer_src(input_tensor[:, :, 3].long()).cuda()\n",
    "\n",
    "        # Handle distance sequence embedding\n",
    "        else:\n",
    "            embedded_tensor = torch.zeros(batch_size, seq_length, self.size).cuda()\n",
    "            start_index = 0\n",
    "            for i, (linear_layer, embedding_layer) in enumerate(zip(self.linear_layers_distance, self.type_layers_distance)):\n",
    "                # Linearly project each feature of the distance sequence\n",
    "                linear_output = linear_layer(input_tensor[:, :, i].view(-1, 1)).cuda()\n",
    "\n",
    "                # Get the embedding dimension for the current feature\n",
    "                embedding_dim = embedding_dims[i]\n",
    "                \n",
    "                # Embed the categorical features in the distance sequence\n",
    "                embedding_output = embedding_layer(type_tensor[:, :, i].long()).cuda()\n",
    "                embedded_tensor[:, :, start_index:start_index + embedding_output.shape[-1]] = embedding_output\n",
    "                start_index = start_index + embedding_output.shape[-1]\n",
    "\n",
    "                # Place the linearly projected distance feature into the tensor\n",
    "                end_index = start_index + embedding_dim\n",
    "                embedded_tensor[:, :, start_index:end_index] = linear_output.view(batch_size, seq_length, embedding_dim).cuda()\n",
    "                start_index = end_index\n",
    "\n",
    "        # Add positional encoding to the embeddings\n",
    "        positional_encoding = self.positional_encoding_src if is_src else self.positional_encoding_distance\n",
    "        embedded_tensor = embedded_tensor + positional_encoding\n",
    "\n",
    "        # Reshape the output tensor and return the final result\n",
    "        embedded_tensor = embedded_tensor.view(batch_size, seq_length, -1).cuda()\n",
    "        return embedded_tensor.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A Multi-Layer Perceptron (MLP) decoder for transforming input features\n",
    "    into desired output representations. The architecture consists of two \n",
    "    fully connected layers with ReLU activation and dropout for regularization.\n",
    "\n",
    "    Args:\n",
    "        input_size (int): The number of input features.\n",
    "        hidden_size (int): The number of units in the hidden layer.\n",
    "        output_size (int): The number of output features.\n",
    "        dropout (float): Dropout probability for regularization.\n",
    "\n",
    "    Methods:\n",
    "        forward(x): Forward pass through the network. Processes the input \n",
    "                    tensor `x` through dropout, activation, and fully connected layers.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, output_size, dropout):\n",
    "        \"\"\"\n",
    "        Initialize the MLPDecoder model.\n",
    "\n",
    "        Args:\n",
    "            input_size (int): The size of the input features.\n",
    "            hidden_size (int): The size of the hidden layer.\n",
    "            output_size (int): The size of the output layer.\n",
    "            dropout (float): Dropout probability for regularization.\n",
    "        \"\"\"\n",
    "        super(MLPDecoder, self).__init__()\n",
    "\n",
    "        # First fully connected layer: input_size -> hidden_size\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size).cuda()\n",
    "        \n",
    "        # Second fully connected layer: hidden_size -> output_size\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size).cuda()\n",
    "        \n",
    "        # Dropout layer for regularization after input\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # ReLU activation after dropout and first FC layer\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # Additional dropout and ReLU activation after the first FC layer\n",
    "        self.dropout1 = nn.Dropout(p=dropout)\n",
    "        self.relu1 = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass through the MLP decoder.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, input_size).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape (batch_size, output_size).\n",
    "        \"\"\"\n",
    "        # Apply dropout to the input\n",
    "        x = self.dropout(x).cuda()\n",
    "        \n",
    "        # Apply ReLU activation\n",
    "        x = self.relu(x).cuda()\n",
    "        \n",
    "        # First fully connected layer\n",
    "        x = self.fc1(x).cuda()\n",
    "        \n",
    "        # Additional dropout after the first FC layer\n",
    "        x = self.dropout1(x).cuda()\n",
    "        \n",
    "        # Additional ReLU activation\n",
    "        x = self.relu1(x).cuda()\n",
    "        \n",
    "        # Second fully connected layer (output layer)\n",
    "        x = self.fc2(x).cuda()\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBase(nn.Module):\n",
    "    \"\"\"\n",
    "    A Transformer-based neural network model for sequence-to-sequence tasks. \n",
    "    This class integrates embedding layers, transformer encoder-decoder blocks, \n",
    "    and a projection layer for output.\n",
    "\n",
    "    Args:\n",
    "        embedding_dims (list[int]): List of embedding dimensions for each feature.\n",
    "        src_len (int): Length of the source sequence.\n",
    "        tgt_len (int): Length of the target sequence.\n",
    "        hidden (int): Hidden layer size for the MLP decoder.\n",
    "        num_layers (int): Number of layers in the encoder and decoder.\n",
    "        num_heads (int): Number of attention heads in the transformer layers.\n",
    "        dropout (float): Dropout probability for regularization (default is 0.1).\n",
    "\n",
    "    Methods:\n",
    "        forward(src, tgt=None, src_mask=None, tgt_mask=None, training=True):\n",
    "            Executes the forward pass of the model. If `training` is True, \n",
    "            the target tensor is used for decoding; otherwise, decoding is done \n",
    "            without a target.\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_dims, src_len, tgt_len, hidden, num_layers, num_heads, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Initialize the TransformerBase model.\n",
    "\n",
    "        Args:\n",
    "            embedding_dims (list[int]): Embedding dimensions for each input feature.\n",
    "            src_len (int): Source sequence length.\n",
    "            tgt_len (int): Target sequence length.\n",
    "            hidden (int): Hidden size for the MLP decoder.\n",
    "            num_layers (int): Number of transformer encoder/decoder layers.\n",
    "            num_heads (int): Number of attention heads in each transformer layer.\n",
    "            dropout (float): Dropout rate for regularization.\n",
    "        \"\"\"\n",
    "        super(TransformerBase, self).__init__()\n",
    "        self.src_len = src_len\n",
    "        self.tgt_len = tgt_len\n",
    "        self.embedding_size = sum(embedding_dims)  # Total embedding size.\n",
    "        self.hidden_size = hidden\n",
    "        self.output_size = len(embedding_dims)  # Number of output dimensions (e.g., x and y coordinates).\n",
    "\n",
    "        # Embedding layer for source and target sequences.\n",
    "        self.embedding_layer = TransformerBaseEmbedding(\n",
    "            embedding_dims, sequence_length_src=src_len, sequence_length_tgt=tgt_len\n",
    "        ).cuda()\n",
    "\n",
    "        # Transformer encoder and decoder layers.\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=self.embedding_size, nhead=num_heads, batch_first=True).cuda()\n",
    "        self.decoder_layer = nn.TransformerDecoderLayer(d_model=self.embedding_size, nhead=num_heads, batch_first=True).cuda()\n",
    "\n",
    "        # Full encoder and decoder using the above layers.\n",
    "        self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers).cuda()\n",
    "        self.decoder = nn.TransformerDecoder(self.decoder_layer, num_layers=num_layers).cuda()\n",
    "\n",
    "        # Linear projection layer for output mapping.\n",
    "        self.linear_output = nn.Linear(self.embedding_size, self.output_size).cuda()\n",
    "\n",
    "        # MLP Decoder for final feature transformation.\n",
    "        self.mlpdecoder = MLPDecoder(self.embedding_size, self.hidden_size, self.output_size, dropout=dropout).cuda()\n",
    "\n",
    "        # Regularization layers.\n",
    "        self.dropout = nn.Dropout(p=dropout).cuda()\n",
    "        self.relu = nn.ReLU().cuda()\n",
    "\n",
    "    def forward(self, src, tgt=None, src_mask=None, tgt_mask=None, training=True):\n",
    "        \"\"\"\n",
    "        Forward pass through the TransformerBase model.\n",
    "\n",
    "        Args:\n",
    "            src (torch.Tensor): Source sequence tensor of shape (batch_size, src_len, embedding_size).\n",
    "            tgt (torch.Tensor, optional): Target sequence tensor of shape (batch_size, tgt_len, embedding_size). Required if training=True.\n",
    "            src_mask (torch.Tensor, optional): Mask for the source sequence (default is None).\n",
    "            tgt_mask (torch.Tensor, optional): Mask for the target sequence (default is None).\n",
    "            training (bool, optional): If True, the model uses the target tensor for decoding (default is True).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape (batch_size, tgt_len, output_size).\n",
    "        \"\"\"\n",
    "        # Embed the source sequence.\n",
    "        src_embedded = self.embedding_layer(src, is_src=True).cuda()\n",
    "        src_embedded = self.dropout(src_embedded).cuda()  # Apply dropout.\n",
    "        src_embedded = self.relu(src_embedded).cuda()  # Apply ReLU activation.\n",
    "\n",
    "        # Pass the source embeddings through the transformer encoder.\n",
    "        memory = self.encoder(src_embedded, mask=src_mask).cuda()\n",
    "\n",
    "        if training:\n",
    "            # Embed the target sequence for training.\n",
    "            tgt_embedded = self.embedding_layer(tgt, is_src=False).cuda()\n",
    "            \n",
    "            # Decode using the target embeddings and encoded memory.\n",
    "            output = self.decoder(tgt_embedded, memory, tgt_mask=tgt_mask).cuda()\n",
    "        else:\n",
    "            # For inference, create a start token tensor for decoding.\n",
    "            start_token = torch.zeros(1, self.tgt_len, src_embedded.shape[2]).cuda()\n",
    "            start_token = start_token.repeat(src.shape[0], 1, 1)  # Repeat for the batch size.\n",
    "\n",
    "            # Perform decoding using the start token and encoded memory.\n",
    "            output = self.decoder(start_token, memory, tgt_mask=tgt_mask).cuda()\n",
    "\n",
    "        # Pass the output through the MLP decoder.\n",
    "        output = self.mlpdecoder(output).cuda()\n",
    "\n",
    "        # Reshape to (batch_size, tgt_len, output_size) for final output.\n",
    "        output = output.view(src.shape[0], self.tgt_len, self.output_size).cuda()\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer Model: Implements a transformer-based architecture for sequence-to-sequence \n",
    "    prediction tasks, particularly for spatial-temporal data processing.\n",
    "\n",
    "    This model uses an encoder-decoder structure with transformer layers and MLP decoders \n",
    "    for output projection.\n",
    "\n",
    "    Args:\n",
    "        src_dim (list): Dimensions of the source input features.\n",
    "        tgt_dim (list): Dimensions of the target output features.\n",
    "        num_agents (int): Number of agents in the dataset.\n",
    "        num_layers (int): Number of transformer layers in the encoder and decoder.\n",
    "        num_heads (int): Number of attention heads in the transformer layers.\n",
    "        hidden (int): Hidden layer size for the MLP decoder.\n",
    "        src_len (int): Length of the source sequence (default=10).\n",
    "        tgt_len (int): Length of the target sequence (default=40).\n",
    "        dropout (float): Dropout probability for regularization (default=0.1).\n",
    "\n",
    "    Methods:\n",
    "        forward(src, tgt=None, src_mask=None, tgt_mask=None, training=True):\n",
    "            Performs a forward pass through the model.\n",
    "    \"\"\"\n",
    "    def __init__(self, src_dim, tgt_dim, num_agents, num_layers, num_heads, hidden, src_len=10, tgt_len=40, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Initialize the Transformer model.\n",
    "\n",
    "        Args:\n",
    "            src_dim (list): Source input feature dimensions.\n",
    "            tgt_dim (list): Target output feature dimensions.\n",
    "            num_agents (int): Number of agents in the dataset.\n",
    "            num_layers (int): Number of encoder and decoder layers.\n",
    "            num_heads (int): Number of attention heads in transformer layers.\n",
    "            hidden (int): Hidden size for the MLP decoder.\n",
    "            src_len (int): Length of the source sequence.\n",
    "            tgt_len (int): Length of the target sequence.\n",
    "            dropout (float): Dropout probability.\n",
    "        \"\"\"\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        # Key parameters for architecture\n",
    "        self.src_len = src_len  # Source sequence length\n",
    "        self.tgt_len = tgt_len  # Target sequence length\n",
    "        self.size = sum(src_dim)  # Total input embedding size\n",
    "        self.hidden = hidden  # Hidden size for MLP decoder\n",
    "        self.output_size = len(tgt_dim)  # Output size (e.g., x and y coordinates)\n",
    "\n",
    "        # Embedding layer for source and target data\n",
    "        self.embedding_layer = TransformerEmbedding(\n",
    "            src_dims=src_dim,\n",
    "            tgt_dims=tgt_dim,\n",
    "            num_agents=num_agents,\n",
    "            sequence_length_src=src_len,\n",
    "            sequence_length_tgt=tgt_len\n",
    "        ).cuda()\n",
    "\n",
    "        # Transformer encoder and decoder layers\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=self.size, nhead=num_heads, batch_first=True).cuda()\n",
    "        self.decoder_layer = nn.TransformerDecoderLayer(d_model=self.size, nhead=num_heads, batch_first=True).cuda()\n",
    "        \n",
    "\n",
    "        # Full encoder and decoder stacks\n",
    "        self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers).cuda()\n",
    "        \n",
    "        self.decoder = nn.TransformerDecoder(self.decoder_layer, num_layers=num_layers).cuda()\n",
    "\n",
    "        # Linear projection layer for output\n",
    "        self.mlpdecoder = MLPDecoder(self.size, self.hidden, self.output_size, dropout=dropout).cuda()\n",
    "\n",
    "        # Regularization and activation layers\n",
    "        self.dropout = nn.Dropout(p=dropout).cuda()\n",
    "        self.relu = nn.ReLU().cuda()\n",
    "\n",
    "    def forward(self, src, tgt=None, src_mask=None, tgt_mask=None, training=True):\n",
    "        \"\"\"\n",
    "        Forward pass of the Transformer model.\n",
    "\n",
    "        Args:\n",
    "            src (torch.Tensor): Source input tensor of shape (batch_size, src_len, src_dim).\n",
    "            tgt (torch.Tensor, optional): Target input tensor of shape (batch_size, tgt_len, tgt_dim).\n",
    "                Required if training is True.\n",
    "            src_mask (torch.Tensor, optional): Attention mask for source input (default=None).\n",
    "            tgt_mask (torch.Tensor, optional): Attention mask for target input (default=None).\n",
    "            training (bool): Indicates whether the model is in training mode (default=True).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape (batch_size, tgt_len, output_dim).\n",
    "        \"\"\"\n",
    "        # Embed the source sequence.\n",
    "        src_embedded = self.embedding_layer(src, is_src=True).cuda()\n",
    "        src_embedded = self.dropout(src_embedded).cuda()  # Apply dropout.\n",
    "        src_embedded = self.relu(src_embedded).cuda()  # Apply ReLU activation.\n",
    "\n",
    "        # Pass the source embeddings through the transformer encoder.\n",
    "        memory = self.encoder(src_embedded, mask=src_mask).cuda()\n",
    "\n",
    "        if training:\n",
    "            # Embed the target sequence for training.\n",
    "            tgt_embedded = self.embedding_layer(tgt, is_src=False).cuda()\n",
    "            \n",
    "            # Decode using the target embeddings and encoded memory.\n",
    "            output = self.decoder(tgt_embedded, memory, tgt_mask=tgt_mask).cuda()\n",
    "        else:\n",
    "            # For inference, create a start token tensor for decoding.\n",
    "            start_token = torch.zeros(1, self.tgt_len, src_embedded.shape[2]).cuda()\n",
    "            start_token = start_token.repeat(src.shape[0], 1, 1)  # Repeat for the batch size.\n",
    "\n",
    "            # Perform decoding using the start token and encoded memory.\n",
    "            output = self.decoder(start_token, memory, tgt_mask=tgt_mask).cuda()\n",
    "\n",
    "        # Pass the output through the MLP decoder.\n",
    "        output = self.mlpdecoder(output).cuda()\n",
    "\n",
    "        # Reshape to (batch_size, tgt_len, output_size) for final output.\n",
    "        output = output.view(src.shape[0], self.tgt_len, self.output_size).cuda()\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class STAR(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    STAR Model: A spatial-temporal attention-based model for sequence prediction tasks.\n",
    "\n",
    "    This model incorporates spatial and temporal transformer encoders, \n",
    "    along with MLP decoders, to predict sequences over a target length.\n",
    "\n",
    "    Args:\n",
    "        embedding_dims (np.array): Dimensions of the input embeddings.\n",
    "        dist_dims (np.array): Dimensions of the distance embeddings.\n",
    "        type_dims (np.array): Dimensions of the type embeddings.\n",
    "        num_types (int): Number of unique types in the type embeddings.\n",
    "        hidden (int): Hidden layer size for the decoders (default=256).\n",
    "        num_layers (int): Number of transformer encoder layers (default=16).\n",
    "        num_heads (int): Number of attention heads in the transformer layers (default=8).\n",
    "        src_len (int): Length of the source sequence.\n",
    "        tgt_len (int): Length of the target sequence.\n",
    "        dropout (float): Dropout probability for regularization (default=0.1).\n",
    "\n",
    "    Methods:\n",
    "        forward(src, distance, type, src_mask=None, dist_key_padding_mask=None):\n",
    "            Performs a forward pass through the model, processing embeddings, encoding \n",
    "            spatial and temporal features, and decoding predictions.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        embedding_dims: np.array, \n",
    "        dist_dims: np.array, \n",
    "        type_dims: np.array, \n",
    "        num_types: int, \n",
    "        hidden: int = 256, \n",
    "        num_layers: int = 16, \n",
    "        num_heads: int = 8, \n",
    "        src_len: int = 10, \n",
    "        tgt_len: int = 40, \n",
    "        dropout: float = 0.1\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the STAR model.\n",
    "\n",
    "        Args:\n",
    "            embedding_dims (np.array): Dimensions for input embeddings.\n",
    "            dist_dims (np.array): Dimensions for distance embeddings.\n",
    "            type_dims (np.array): Dimensions for type embeddings.\n",
    "            num_types (int): Number of unique types for type embeddings.\n",
    "            hidden (int): Hidden layer size for decoders.\n",
    "            num_layers (int): Number of layers in transformer encoders.\n",
    "            num_heads (int): Number of attention heads in transformers.\n",
    "            src_len (int): Length of the source sequence.\n",
    "            tgt_len (int): Length of the target sequence.\n",
    "            dropout (float): Dropout probability for regularization.\n",
    "        \"\"\"\n",
    "        super(STAR, self).__init__()\n",
    "\n",
    "        # Define key architectural parameters\n",
    "        self.embedding_size = sum(embedding_dims)  # Total embedding size after concatenation.\n",
    "        self.output_size = 2  # Output size (e.g., x and y coordinates).\n",
    "        self.dropout_prob = dropout  # Dropout rate.\n",
    "        self.src_len = src_len  # Source sequence length.\n",
    "        self.tgt_len = tgt_len  # Target sequence length.\n",
    "        self.hidden_size = hidden  # Hidden size for decoders.\n",
    "\n",
    "        # Embedding layer for input, distance, and type data.\n",
    "        self.embedding_layer = STAREmbedding(embedding_dims, dist_dims, type_dims, num_types, src_len).cuda()\n",
    "\n",
    "        # Transformer encoder layers for temporal and spatial data.\n",
    "        self.temporal_encoder_layer = nn.TransformerEncoderLayer(d_model=self.embedding_size, nhead=num_heads, batch_first=True).cuda()\n",
    "        self.spatial_encoder_layer = nn.TransformerEncoderLayer(d_model=self.embedding_size, nhead=num_heads, batch_first=True).cuda()\n",
    "\n",
    "        # Stacked transformer encoders for spatial and temporal processing.\n",
    "        self.spatial_encoder_1 = nn.TransformerEncoder(self.spatial_encoder_layer, num_layers).cuda()\n",
    "        self.spatial_encoder_2 = nn.TransformerEncoder(self.spatial_encoder_layer, num_layers).cuda()\n",
    "        self.temporal_encoder_1 = nn.TransformerEncoder(self.temporal_encoder_layer, num_layers).cuda()\n",
    "        self.temporal_encoder_2 = nn.TransformerEncoder(self.temporal_encoder_layer, num_layers).cuda()\n",
    "\n",
    "        # Decoders for transforming encoded features to output.\n",
    "        self.decoder1 = MLPDecoder(self.embedding_size, self.hidden_size, self.output_size, dropout=dropout).cuda()\n",
    "        self.decoder2 = MLPDecoder(self.embedding_size, self.hidden_size, self.output_size, dropout=dropout).cuda()\n",
    "        self.decoder3 = MLPDecoder(self.embedding_size, self.hidden_size, self.output_size, dropout=dropout).cuda()\n",
    "        self.decoder4 = MLPDecoder(self.embedding_size, self.hidden_size, self.output_size, dropout=dropout).cuda()\n",
    "\n",
    "        # Fusion layer to combine spatial and temporal features.\n",
    "        self.fusion_layer = nn.Linear(self.embedding_size * 2, self.embedding_size).cuda()\n",
    "\n",
    "        # Regularization and activation layers.\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(p=dropout)\n",
    "        self.relu1 = nn.ReLU()\n",
    "\n",
    "    def forward(self, src, distance, type, src_mask=None, dist_key_padding_mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass of the STAR model.\n",
    "\n",
    "        Args:\n",
    "            src (torch.Tensor): Source input tensor of shape (batch_size, src_len, embedding_dims).\n",
    "            distance (torch.Tensor): Distance input tensor of shape (batch_size, src_len, dist_dims).\n",
    "            type (torch.Tensor): Type input tensor of shape (batch_size, src_len, type_dims).\n",
    "            src_mask (torch.Tensor, optional): Mask for source input (default=None).\n",
    "            dist_key_padding_mask (torch.Tensor, optional): Mask for distance input (default=None).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape (batch_size, tgt_len, output_size).\n",
    "        \"\"\"\n",
    "        # Temporal embedding for source data.\n",
    "        src_temporal_embedded = self.embedding_layer(src, is_src=True).cuda()\n",
    "        src_temporal_embedded = self.dropout(src_temporal_embedded).cuda()  # Apply dropout.\n",
    "        src_temporal_embedded = self.relu(src_temporal_embedded).cuda()  # Apply ReLU activation.\n",
    "\n",
    "        # Spatial embedding for distance data.\n",
    "        src_dist_embedding = self.embedding_layer(distance, is_src=False, src_tensor=src, type_tensor=type).cuda()\n",
    "        src_dist_embedded = self.dropout1(src_dist_embedding).cuda()  # Apply dropout.\n",
    "        src_dist_embedded = self.relu1(src_dist_embedded).cuda()  # Apply ReLU activation.\n",
    "\n",
    "        # Process embeddings through spatial and temporal encoders.\n",
    "        spatial_input_embedded = self.spatial_encoder_1(src_dist_embedded, mask=src_mask).cuda()\n",
    "        temporal_input_embedded = self.temporal_encoder_1(src_temporal_embedded, mask=src_mask).cuda()\n",
    "\n",
    "        # Fuse spatial and temporal features.\n",
    "        fusion_feat = torch.cat((temporal_input_embedded, spatial_input_embedded), dim=2).cuda()\n",
    "        fusion_feat = self.fusion_layer(fusion_feat).cuda()  # Apply linear fusion layer.\n",
    "\n",
    "        # Further processing through secondary encoders.\n",
    "        spatial_output = self.spatial_encoder_2(fusion_feat).cuda()\n",
    "        temporal_output = self.temporal_encoder_2(spatial_output).cuda()\n",
    "\n",
    "        # Reshape output for decoding.\n",
    "        temporal_output = temporal_output.reshape(\n",
    "            temporal_output.shape[0] * temporal_output.shape[1], temporal_output.shape[2]\n",
    "        )\n",
    "\n",
    "        # Decode spatial-temporal features using decoders.\n",
    "        output1 = self.decoder1(temporal_output).cuda()\n",
    "        output2 = self.decoder2(temporal_output).cuda()\n",
    "        output3 = self.decoder3(temporal_output).cuda()\n",
    "        output4 = self.decoder4(temporal_output).cuda()\n",
    "\n",
    "        # Concatenate decoder outputs and reshape.\n",
    "        output = torch.cat([output1, output2, output3, output4], dim=1)\n",
    "        output = output.reshape(src.shape[0], self.tgt_len, self.output_size).cuda()\n",
    "\n",
    "        return output.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAESTAR(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    SAESTAR Model: A spatial-temporal transformer-based model for sequence prediction tasks.\n",
    "\n",
    "    This model integrates spatial and temporal encoding layers with a multi-layer perceptron (MLP) decoder. \n",
    "    It fuses spatial and temporal features to produce predictions over a target sequence length.\n",
    "\n",
    "    Args:\n",
    "        src_dims (list[int]): Dimensions of source embeddings.\n",
    "        dist_dims (list[int]): Dimensions of distance embeddings.\n",
    "        type_dims (list[int]): Dimensions of type embeddings.\n",
    "        num_types (int): Number of unique types in the type embedding.\n",
    "        num_types_dist (int): Number of unique types in the distance embedding.\n",
    "        hidden (int): Hidden layer size for the decoders (default=256).\n",
    "        num_layers (int): Number of layers in the transformer encoders (default=16).\n",
    "        num_heads (int): Number of attention heads in the transformer layers (default=8).\n",
    "        src_len (int): Length of the source sequence.\n",
    "        tgt_len (int): Length of the target sequence.\n",
    "        dropout (float): Dropout probability for regularization (default=0.1).\n",
    "\n",
    "    Methods:\n",
    "        forward(src, distance, type, src_mask=None, dist_key_padding_mask=None):\n",
    "            Executes the forward pass of the model, embedding the input data,\n",
    "            encoding spatial and temporal features, and decoding the output.\n",
    "    \"\"\"\n",
    "    def __init__(self, src_dims, dist_dims, type_dims, num_types, num_types_dist, hidden=256, num_layers=16, num_heads=8, src_len=10, tgt_len=40, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Initialize the SAESTAR model.\n",
    "\n",
    "        Args:\n",
    "            src_dims (list[int]): Dimensions for the source embeddings.\n",
    "            dist_dims (list[int]): Dimensions for the distance embeddings.\n",
    "            type_dims (list[int]): Dimensions for the type embeddings.\n",
    "            num_types (int): Total number of unique types for type embeddings.\n",
    "            num_types_dist (int): Total number of unique types for distance embeddings.\n",
    "            hidden (int): Hidden size for the decoders.\n",
    "            num_layers (int): Number of layers for each transformer encoder.\n",
    "            num_heads (int): Number of attention heads in each transformer layer.\n",
    "            src_len (int): Length of the source sequence.\n",
    "            tgt_len (int): Length of the target sequence.\n",
    "            dropout (float): Dropout probability for regularization.\n",
    "        \"\"\"\n",
    "        super(SAESTAR, self).__init__()\n",
    "\n",
    "        # Model architecture parameters.\n",
    "        self.embedding_size = sum(src_dims)  # Total size of the input embeddings.\n",
    "        self.output_size = 2  # Output dimensions (e.g., x and y coordinates).\n",
    "        self.dropout_prob = dropout  # Dropout probability.\n",
    "        self.src_len = src_len  # Source sequence length.\n",
    "        self.tgt_len = tgt_len  # Target sequence length.\n",
    "        self.hidden_size = hidden  # Hidden layer size.\n",
    "\n",
    "        # Embedding layer for source, distance, and type inputs.\n",
    "        self.embedding_layer = SAESTAREmbedding(src_dims, dist_dims, type_dims, num_types, num_types_dist, src_len).cuda()\n",
    "\n",
    "        # Transformer encoder layers for spatial and temporal features.\n",
    "        self.temporal_encoder_layer = nn.TransformerEncoderLayer(d_model=self.embedding_size, nhead=num_heads, batch_first=True).cuda()\n",
    "        self.spatial_encoder_layer = nn.TransformerEncoderLayer(d_model=self.embedding_size, nhead=num_heads, batch_first=True).cuda()\n",
    "\n",
    "        # Stacked transformer encoders for spatial and temporal processing.\n",
    "        self.spatial_encoder_1 = nn.TransformerEncoder(self.spatial_encoder_layer, num_layers).cuda()\n",
    "        self.spatial_encoder_2 = nn.TransformerEncoder(self.spatial_encoder_layer, num_layers).cuda()\n",
    "        self.temporal_encoder_1 = nn.TransformerEncoder(self.temporal_encoder_layer, num_layers).cuda()\n",
    "        self.temporal_encoder_2 = nn.TransformerEncoder(self.temporal_encoder_layer, num_layers).cuda()\n",
    "\n",
    "        # Decoders for spatial-temporal feature projection.\n",
    "        self.decoder1 = MLPDecoder(self.embedding_size, self.hidden_size, self.output_size, dropout=dropout).cuda()\n",
    "        self.decoder2 = MLPDecoder(self.embedding_size, self.hidden_size, self.output_size, dropout=dropout).cuda()\n",
    "        self.decoder3 = MLPDecoder(self.embedding_size, self.hidden_size, self.output_size, dropout=dropout).cuda()\n",
    "        self.decoder4 = MLPDecoder(self.embedding_size, self.hidden_size, self.output_size, dropout=dropout).cuda()\n",
    "\n",
    "        # Fusion layer for combining spatial and temporal features.\n",
    "        self.fusion_layer = nn.Linear(self.embedding_size * 2, self.embedding_size).cuda()\n",
    "\n",
    "        # Regularization and activation layers.\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(p=dropout)\n",
    "        self.relu1 = nn.ReLU()\n",
    "\n",
    "    def forward(self, src, distance, type, src_mask=None, dist_key_padding_mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass through the SAESTAR model.\n",
    "\n",
    "        Args:\n",
    "            src (torch.Tensor): Source sequence tensor of shape (batch_size, src_len, src_dims).\n",
    "            distance (torch.Tensor): Distance tensor of shape (batch_size, src_len, dist_dims).\n",
    "            type (torch.Tensor): Type tensor of shape (batch_size, src_len, type_dims).\n",
    "            src_mask (torch.Tensor, optional): Mask for the source sequence (default=None).\n",
    "            dist_key_padding_mask (torch.Tensor, optional): Mask for the distance tensor (default=None).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape (batch_size, tgt_len, output_size).\n",
    "        \"\"\"\n",
    "        # Embed the source sequence for temporal encoding.\n",
    "        src_temporal_embedded = self.embedding_layer(src, is_src=True).cuda()\n",
    "        src_temporal_embedded = self.dropout(src_temporal_embedded).cuda()  # Apply dropout.\n",
    "        src_temporal_embedded = self.relu(src_temporal_embedded).cuda()  # Apply ReLU activation.\n",
    "\n",
    "        # Embed the distance sequence for spatial encoding.\n",
    "        src_dist_embedding = self.embedding_layer(distance, is_src=False, src_tensor=src, type_tensor=type).cuda()\n",
    "        src_dist_embedded = self.dropout1(src_dist_embedding).cuda()  # Apply dropout.\n",
    "        src_dist_embedded = self.relu1(src_dist_embedded).cuda()  # Apply ReLU activation.\n",
    "\n",
    "        # Process spatial and temporal embeddings through respective encoders.\n",
    "        spatial_input_embedded = self.spatial_encoder_1(src_dist_embedded, mask=src_mask).cuda()\n",
    "        temporal_input_embedded = self.temporal_encoder_1(src_temporal_embedded, mask=src_mask).cuda()\n",
    "\n",
    "        # Fuse spatial and temporal features.\n",
    "        fusion_feat = torch.cat((temporal_input_embedded, spatial_input_embedded), dim=2).cuda()\n",
    "        fusion_feat = self.fusion_layer(fusion_feat).cuda()  # Combine features using the fusion layer.\n",
    "\n",
    "        # Process the fused features through secondary encoders.\n",
    "        spatial_output = self.spatial_encoder_2(fusion_feat).cuda()\n",
    "        temporal_output = self.temporal_encoder_2(spatial_output).cuda()\n",
    "\n",
    "        # Reshape the temporal output for decoding.\n",
    "        temporal_output = temporal_output.reshape(\n",
    "            temporal_output.shape[0] * temporal_output.shape[1], temporal_output.shape[2]\n",
    "        )\n",
    "\n",
    "        # Decode the temporal output through multiple decoders.\n",
    "        output1 = self.decoder1(temporal_output).cuda()\n",
    "        output2 = self.decoder2(temporal_output).cuda()\n",
    "        output3 = self.decoder3(temporal_output).cuda()\n",
    "        output4 = self.decoder4(temporal_output).cuda()\n",
    "\n",
    "        # Concatenate the outputs from all decoders.\n",
    "        output = torch.cat([output1, output2, output3, output4], dim=1)\n",
    "        output = output.reshape(src.shape[0], self.tgt_len, self.output_size).cuda()\n",
    "\n",
    "        return output.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scaler:\n",
    "    \"\"\"\n",
    "    A class to scale and unscale data using Min-Max scaling. It supports scaling of source (src),\n",
    "    target (tgt), and distance (distance) data, with optional spatial scaling.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, train_data : dict, model_name : str=None, spatial=False, size : int=None):\n",
    "        \"\"\"\n",
    "        Initializes the Scaler class, fitting separate scalers for different data types (source, target, distance).\n",
    "        \n",
    "        Parameters:\n",
    "        train_data (dict): A dictionary containing training data with keys 'src', 'tgt', and 'distance'.\n",
    "        spatial (bool): If True, a scaler for distance data is also created (default: False).\n",
    "        \"\"\"\n",
    "        self.spatial = False  # Default spatial scaling is turned off\n",
    "        # Create and fit the scaler for the source data (first 3 columns of 'src')\n",
    "        self.src_scaler = self.create_scaler(train_data['src'][:, :, :3])\n",
    "        # Create and fit the scaler for the target data (first 2 columns of 'tgt')\n",
    "        self.tgt_scaler = self.create_scaler(train_data['tgt'][:, :, :2])\n",
    "        # If spatial scaling is enabled, create and fit the scaler for distance data\n",
    "        if spatial:\n",
    "            if model_name == \"STAR\":\n",
    "                self.dist_scaler = self.create_scaler(train_data['distance'][:, :, :size])\n",
    "            else:\n",
    "                self.dist_scaler = self.create_scaler(train_data['distance'])\n",
    "                \n",
    "    def create_scaler(self, data):\n",
    "        \"\"\"\n",
    "        Creates and fits a MinMaxScaler to the provided data.\n",
    "        \n",
    "        Parameters:\n",
    "        data (numpy.ndarray): The data to fit the scaler on.\n",
    "        \n",
    "        Returns:\n",
    "        MinMaxScaler: A fitted MinMaxScaler instance.\n",
    "        \"\"\"\n",
    "        # Get the shape of the input data\n",
    "        shape = data.shape\n",
    "        # Instantiate a MinMaxScaler\n",
    "        scaler = MinMaxScaler()\n",
    "        # Convert data to numpy if it's a tensor\n",
    "        np_data = np.array(data)\n",
    "        # Flatten the data to 2D for scaling\n",
    "        flat_data = np_data.reshape(shape[0] * shape[1], shape[2])\n",
    "        # Clip the data to avoid extreme values that could distort scaling\n",
    "        clipped_data = np.clip(np.array(flat_data), -1e6, 1e6)\n",
    "        # Fit and return the scaler on the clipped data\n",
    "        return scaler.fit(clipped_data)\n",
    "\n",
    "    def scale(self, data, scaler_type: str):\n",
    "        \"\"\"\n",
    "        Scales the input data using the specified scaler type (src, tgt, or distance).\n",
    "        \n",
    "        Parameters:\n",
    "        data (torch.Tensor): The data to be scaled (tensor format).\n",
    "        scaler_type (str): The type of scaler to use ('src', 'tgt', or 'distance').\n",
    "        \n",
    "        Returns:\n",
    "        torch.Tensor: Scaled data in the original shape.\n",
    "        \"\"\"\n",
    "        # Select the appropriate scaler based on the input type\n",
    "        if scaler_type == \"src\":\n",
    "            scaler = self.src_scaler\n",
    "        elif scaler_type == \"tgt\":\n",
    "            scaler = self.tgt_scaler\n",
    "        elif scaler_type == \"dist\":\n",
    "            scaler = self.dist_scaler\n",
    "        \n",
    "        # Get the shape of the input data\n",
    "        shape = data.shape\n",
    "        # Flatten the data into 2D for scaling\n",
    "        data_flat = data.view(data.shape[0] * data.shape[1], data.shape[2])\n",
    "        # Clip the data to avoid extreme values that could distort scaling\n",
    "        data_clipped = np.clip(np.array(data_flat.cpu()), -1e6, 1e6)\n",
    "        # Transform the data using the fitted scaler\n",
    "        data_scaled = torch.tensor(scaler.transform(data_clipped)).cuda()\n",
    "        # Reshape the data back to its original shape\n",
    "        reshaped_data = data_scaled.reshape(shape)\n",
    "        return reshaped_data\n",
    "\n",
    "    def unscale(self, data, scaler_type: str, original_shape):\n",
    "        \"\"\"\n",
    "        Reverses the scaling transformation applied to the data, returning the data to its original scale.\n",
    "        \n",
    "        Parameters:\n",
    "        data (torch.Tensor): The data to be unscaled (scaled data in tensor format).\n",
    "        scaler_type (str): The type of scaler to use for unscaling ('src', 'tgt', or 'distance').\n",
    "        original_shape (tuple): The original shape of the data to reshape after unscaling.\n",
    "        \n",
    "        Returns:\n",
    "        torch.Tensor: Unscaled data in its original shape.\n",
    "        \"\"\"\n",
    "        # Select the appropriate scaler based on the input type\n",
    "        if scaler_type == \"src\":\n",
    "            scaler = self.src_scaler\n",
    "        elif scaler_type == \"tgt\":\n",
    "            scaler = self.tgt_scaler\n",
    "        else:\n",
    "            scaler = self.dist_scaler\n",
    "        # Flatten the data for inverse transformation\n",
    "        data_flatten = data.reshape(data.shape[0] * data.shape[1], data.shape[2])\n",
    "        # Inverse transform the data to the original scale\n",
    "        data_unscaled = torch.tensor(scaler.inverse_transform(np.array(data_flatten.cpu()))).cuda()\n",
    "        # Reshape the unscaled data back to the original shape\n",
    "        return data_unscaled.reshape(original_shape)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experiment:\n",
    "    \"\"\"\n",
    "    A class to handle the training and evaluation of various models for time series prediction.\n",
    "\n",
    "    Args:\n",
    "        scaler (Scaler): A Scaler object to normalize input data.\n",
    "        model_name (str): The model architecture to use. Choices: 'Base', 'Transformer', 'STAR', 'SAESTAR'.\n",
    "        src_len (int): The length of the source sequence.\n",
    "        tgt_len (int): The length of the target sequence.\n",
    "        num_types (int, optional): Number of types (used in SAESTAR model). Defaults to None.\n",
    "        graph_dims (int, optional): Dimension of the graph (used in STAR and SAESTAR models). Defaults to None.\n",
    "        layers (int, optional): Number of layers in the model. Defaults to 16.\n",
    "        heads (int, optional): Number of attention heads. Defaults to 8.\n",
    "        hidden_size (int, optional): The hidden layer size. Defaults to 256.\n",
    "        dropout (float, optional): The dropout rate for regularization. Defaults to 0.1.\n",
    "        lr (float, optional): The learning rate for the optimizer. Defaults to 0.000015.\n",
    "\n",
    "    Attributes:\n",
    "        scaler (Scaler): The Scaler object for input normalization.\n",
    "        model_type (str): The type of model to use ('Base', 'Transformer', 'STAR', 'SAESTAR').\n",
    "        model (nn.Module): The model object (e.g., Transformer, STAR, etc.).\n",
    "        optimizer (Adam): The optimizer used for training.\n",
    "        criterion (nn.Module): The loss function (MSELoss).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        scaler: Scaler,\n",
    "        model_name: str, \n",
    "        src_len: int, \n",
    "        tgt_len: int, \n",
    "        num_types: int = None, \n",
    "        graph_dims: int = None, \n",
    "        layers: int = 16, \n",
    "        heads: int = 8, \n",
    "        hidden_size: int = 256, \n",
    "        dropout: float = 0.1, \n",
    "        lr: float = 0.000015,\n",
    "        epochs : int = 10,\n",
    "        location_name : str='',\n",
    "        earlystopping : int=30\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes the Experiment class, selects the appropriate model, and prepares the optimizer.\n",
    "\n",
    "        Args are as described in the class docstring.\n",
    "        \"\"\"\n",
    "        self.scaler = scaler\n",
    "        self.epochs = epochs\n",
    "        self.model_type = model_name\n",
    "        self.heads = heads\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self._name = location_name\n",
    "        self.early_stopping_patience = earlystopping\n",
    "        self.optimizer = None\n",
    "        self.model = None\n",
    "        \n",
    "        # Select the model architecture based on the model_name\n",
    "        if model_name == \"Base\":\n",
    "            self.model = self.base_model(src_len, tgt_len, hidden_size, layers, heads, dropout)\n",
    "        if model_name == \"Transformer\":\n",
    "            self.model = self.transformer_model(src_len, tgt_len, hidden_size, layers, heads, dropout)\n",
    "        if model_name == \"STAR\":\n",
    "            self.model = self.star_model(src_len, tgt_len, graph_dims, hidden_size, layers, heads, dropout)\n",
    "        if model_name == \"SAESTAR\":\n",
    "            self.model = self.saestar_model(src_len, tgt_len, graph_dims, num_types, hidden_size, layers, heads, dropout)\n",
    "        \n",
    "        # Initialize the optimizer\n",
    "        self.optimizer = Adam(self.model.parameters(), lr=lr)\n",
    "\n",
    "    def create_mask(self, batch_size, sequence_length):\n",
    "        \"\"\"\n",
    "        Creates a look-ahead mask for the sequence to prevent attending to future tokens.\n",
    "        \n",
    "        Args:\n",
    "            batch_size (int): The size of the batch (number of sequences).\n",
    "            sequence_length (int): The length of the sequence to create the mask for.\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: The look-ahead mask with shape (batch_size, sequence_length, sequence_length).\n",
    "        \"\"\"\n",
    "        \n",
    "        # Create a look-ahead mask (upper triangular matrix with 1s above the diagonal)\n",
    "        look_ahead_mask = torch.triu(torch.ones(sequence_length, sequence_length), diagonal=1).bool()\n",
    "        \n",
    "        # Expand the mask to match the batch size\n",
    "        look_ahead_mask = look_ahead_mask.unsqueeze(0).expand(batch_size * self.heads, -1, -1)  # Shape: (batch_size, sequence_length, sequence_length)\n",
    "        return look_ahead_mask\n",
    "    \n",
    "   \n",
    "    def generate_embedded_dim(self, total, array_length):\n",
    "        \"\"\"\n",
    "        Generates a list of dimensions that sum up to a total value, ensuring the sum of the list equals the total.\n",
    "        \n",
    "        Args:\n",
    "            total (int): The total value to split into an array.\n",
    "            array_length (int): The length of the array to generate.\n",
    "        \n",
    "        Returns:\n",
    "            list: A list of dimensions.\n",
    "        \"\"\"\n",
    "        initial_value = total // array_length\n",
    "        remainder = total % array_length\n",
    "        array = [initial_value] * array_length\n",
    "        array[-1] += remainder\n",
    "        \n",
    "        return torch.tensor(array, dtype=torch.int).cuda()\n",
    "    \n",
    "    def base_model(self, src, tgt, hidden, layers, heads, dropout) -> nn.Module:\n",
    "        \"\"\"\n",
    "        Creates and returns a Base model.\n",
    "        \n",
    "        Args:\n",
    "            src (int): The source sequence length.\n",
    "            tgt (int): The target sequence length.\n",
    "            hidden (int): The hidden layer size.\n",
    "            layers (int): The number of layers in the model.\n",
    "            heads (int): The number of attention heads.\n",
    "            dropout (float): The dropout rate.\n",
    "        \n",
    "        Returns:\n",
    "            nn.Module: The instantiated Base model.\n",
    "        \"\"\"\n",
    "        embedding_dims = [128, 128]\n",
    "        return TransformerBase(\n",
    "            embedding_dims, \n",
    "            src_len=src, \n",
    "            tgt_len=tgt, \n",
    "            hidden=hidden, \n",
    "            num_layers=layers, \n",
    "            num_heads=heads, \n",
    "            dropout=dropout\n",
    "        )\n",
    "    \n",
    "    def transformer_model(self, src, tgt, hidden, layers, heads, dropout) -> nn.Module:\n",
    "        \"\"\"\n",
    "        Creates and returns a Transformer model.\n",
    "        \n",
    "        Args:\n",
    "            src (int): The source sequence length.\n",
    "            tgt (int): The target sequence length.\n",
    "            hidden (int): The hidden layer size.\n",
    "            layers (int): The number of layers in the model.\n",
    "            heads (int): The number of attention heads.\n",
    "            dropout (float): The dropout rate.\n",
    "        \n",
    "        Returns:\n",
    "            nn.Module: The instantiated Transformer model.\n",
    "        \"\"\"\n",
    "        embedding_dims_src = [94, 94, 64, 4]\n",
    "        embedding_dims_tgt = [128, 128]   \n",
    "        return Transformer(\n",
    "            embedding_dims_src, \n",
    "            embedding_dims_tgt, \n",
    "            num_agents=4, \n",
    "            num_layers=layers, \n",
    "            num_heads=heads, \n",
    "            hidden=hidden, \n",
    "            src_len=src, \n",
    "            tgt_len=tgt, \n",
    "            dropout=dropout\n",
    "        )\n",
    "\n",
    "    def star_model(self, src, tgt, graph_dims, hidden, layers, heads, dropout) -> nn.Module:\n",
    "        \"\"\"\n",
    "        Creates and returns a STAR model.\n",
    "        \n",
    "        Args:\n",
    "            src (int): The source sequence length.\n",
    "            tgt (int): The target sequence length.\n",
    "            graph_dims (int): The graph dimensions.\n",
    "            hidden (int): The hidden layer size.\n",
    "            layers (int): The number of layers in the model.\n",
    "            heads (int): The number of attention heads.\n",
    "            dropout (float): The dropout rate.\n",
    "        \n",
    "        Returns:\n",
    "            nn.Module: The instantiated STAR model.\n",
    "        \"\"\"\n",
    "        src_dims = [94, 94, 64, 4]\n",
    "        total = sum(src_dims)\n",
    "        dist_dims = self.generate_embedded_dim(total / 2, graph_dims) \n",
    "        type_dims = self.generate_embedded_dim(total / 2, graph_dims)\n",
    "        return STAR(src_dims, dist_dims, type_dims, 4, hidden, layers, heads, src, tgt, dropout)\n",
    "\n",
    "    def saestar_model(self, src, tgt, graph_dims, num_types, hidden, layers, heads, dropout) -> nn.Module:\n",
    "        \"\"\"\n",
    "        Creates and returns a SAESTAR model.\n",
    "        \n",
    "        Args:\n",
    "            src (int): The source sequence length.\n",
    "            tgt (int): The target sequence length.\n",
    "            graph_dims (int): The graph dimensions.\n",
    "            num_types (int): The number of types in the model.\n",
    "            hidden (int): The hidden layer size.\n",
    "            layers (int): The number of layers in the model.\n",
    "            heads (int): The number of attention heads.\n",
    "            dropout (float): The dropout rate.\n",
    "        \n",
    "        Returns:\n",
    "            nn.Module: The instantiated SAESTAR model.\n",
    "        \"\"\"\n",
    "        src_dims = [94, 94, 64, 4]\n",
    "        total = sum(src_dims)\n",
    "        dist_dims = self.generate_embedded_dim(total / 2, graph_dims) \n",
    "        type_dims = self.generate_embedded_dim(total / 2, graph_dims) \n",
    "        return SAESTAR(src_dims, dist_dims, type_dims, 4, num_types, hidden, layers, heads, src, tgt, dropout)\n",
    "    \n",
    "    def train(self, train_loader: DataLoader, epoch: int) -> float:\n",
    "        \"\"\"\n",
    "        Trains the model for one epoch using the provided DataLoader.\n",
    "\n",
    "        Args:\n",
    "            train_loader (DataLoader): The DataLoader for the training dataset.\n",
    "            epoch (int): The current epoch number.\n",
    "\n",
    "        Returns:\n",
    "            float: The total training loss for the epoch.\n",
    "        \"\"\"\n",
    "        # Set the model to training mode\n",
    "        self.model.train()\n",
    "        train_loss = 0.0  # Initialize training loss\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{self.epochs}\")\n",
    "\n",
    "\n",
    "        # Iterate over batches in the training data\n",
    "        for batch_idx, batch in enumerate(progress_bar):\n",
    "            self.optimizer.zero_grad()  # Zero gradients before backward pass\n",
    "            \n",
    "            # Handle different model types\n",
    "            if self.model_type == 'Base':\n",
    "                inputs, targets = batch['src'].cuda(), batch['tgt'].cuda()\n",
    "                src_mask = self.create_mask(inputs.shape[0], inputs.shape[1])\n",
    "                tgt_mask = self.create_mask(targets.shape[0], targets.shape[1])\n",
    "                inputs = self.scaler.scale(inputs[:, :, :3], \"src\")\n",
    "                targets = self.scaler.scale(targets[:, :, :2], \"tgt\")\n",
    "                outputs = self.model(inputs[:, :, :2], targets, src_mask=src_mask.cuda(), tgt_mask=tgt_mask.cuda())\n",
    "            \n",
    "            elif self.model_type == 'Transformer':\n",
    "                inputs, targets = batch['src'].cuda(), batch['tgt'].cuda()\n",
    "                src_mask = self.create_mask(inputs.shape[0], inputs.shape[1])\n",
    "                tgt_mask = self.create_mask(targets.shape[0], targets.shape[1])\n",
    "                scaled_inputs = self.scaler.scale(inputs[:, :, :3], \"src\")\n",
    "                targets = self.scaler.scale(targets[:, :, :2], \"tgt\")\n",
    "                inputs = torch.cat((scaled_inputs, inputs[:, :, 4:].cuda()), dim=2)\n",
    "                outputs = self.model(inputs, targets, src_mask=src_mask.cuda(), tgt_mask=tgt_mask.cuda())\n",
    "            \n",
    "            elif self.model_type in ['STAR', 'SAESTAR']:\n",
    "                inputs, targets, distances, distance_types = batch['src'].cuda(), batch['tgt'].cuda(), batch['distance'].cuda(), batch['type'].cuda()\n",
    "                src_mask = self.create_mask(inputs.shape[0], inputs.shape[1])\n",
    "                scaled_inputs = self.scaler.scale(inputs[:, :, :3], \"src\")\n",
    "                targets = self.scaler.scale(targets[:, :, :2], \"tgt\")\n",
    "                distances = self.scaler.scale(distances, \"dist\")\n",
    "                inputs = torch.cat((scaled_inputs, inputs[:, :, 4:].cuda()), dim=2)\n",
    "                outputs = self.model(inputs.type(torch.float32), distances.type(torch.float32), distance_types, src_mask=src_mask.cuda())\n",
    "\n",
    "            # Compute loss, backpropagate, and optimize\n",
    "            loss = self.criterion(outputs.type(torch.float32), targets.type(torch.float32))\n",
    "            loss.backward()  # Backpropagation\n",
    "            self.optimizer.step()  # Optimization step\n",
    "            \n",
    "            # Accumulate loss and update progress bar\n",
    "            train_loss += loss.item()\n",
    "            progress_bar.set_postfix({'Training Loss': train_loss / ((batch_idx + 1) * len(inputs))})\n",
    "\n",
    "        return train_loss\n",
    "\n",
    "    \n",
    "    def eval(self, valid_loader: DataLoader) -> float:\n",
    "        \"\"\"\n",
    "        Evaluates the model on a validation set.\n",
    "\n",
    "        Args:\n",
    "            valid_loader (DataLoader): The DataLoader for the validation dataset.\n",
    "\n",
    "        Returns:\n",
    "            float: The average validation loss for the validation set.\n",
    "        \"\"\"\n",
    "        # Set the model to evaluation mode\n",
    "        self.model.eval()\n",
    "        valid_loss = 0.0\n",
    "\n",
    "        # No gradient computation during validation\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, batch in enumerate(valid_loader):\n",
    "                # Handle different model types and prepare inputs/targets accordingly\n",
    "                if self.model_type == 'Base':\n",
    "                    inputs, targets = batch['src'].cuda(), batch['tgt'].cuda()\n",
    "                    src_mask = self.create_mask(inputs.shape[0], inputs.shape[1])\n",
    "                    tgt_mask = self.create_mask(targets.shape[0], targets.shape[1])\n",
    "                    inputs = self.scaler.scale(inputs[:, :, :3], \"src\")\n",
    "                    targets = self.scaler.scale(targets[:, :, :2], \"tgt\")\n",
    "                    outputs = self.model(inputs[:, :, :2], targets, src_mask=src_mask.cuda(), tgt_mask=tgt_mask.cuda())\n",
    "                \n",
    "                elif self.model_type == 'Transformer':\n",
    "                    inputs, targets = batch['src'].cuda(), batch['tgt'].cuda()\n",
    "                    src_mask = self.create_mask(inputs.shape[0], inputs.shape[1])\n",
    "                    tgt_mask = self.create_mask(targets.shape[0], targets.shape[1])\n",
    "                    scaled_inputs = self.scaler.scale(inputs[:, :, :3], \"src\")\n",
    "                    targets = self.scaler.scale(targets[:, :, :2], \"tgt\")\n",
    "                    inputs = torch.cat((scaled_inputs, inputs[:, :, 4:].cuda()), dim=2)\n",
    "                    outputs = self.model(inputs, targets, src_mask=src_mask.cuda(), tgt_mask=tgt_mask.cuda())\n",
    "                \n",
    "                elif self.model_type in ['STAR', 'SAESTAR']:\n",
    "                    inputs, targets, distances, distance_types = batch['src'].cuda(), batch['tgt'].cuda(), batch['distance'].cuda(), batch['type'].cuda()\n",
    "                    src_mask = self.create_mask(inputs.shape[0], inputs.shape[1])\n",
    "                    scaled_inputs = self.scaler.scale(inputs[:, :, :3], \"src\")\n",
    "                    targets = self.scaler.scale(targets[:, :, :2], \"tgt\")\n",
    "                    distances = self.scaler.scale(distances, \"dist\")\n",
    "                    inputs = torch.cat((scaled_inputs, inputs[:, :, 4:].cuda()), dim=2)\n",
    "                    outputs = self.model(inputs.type(torch.float32), distances.type(torch.float32), distance_types, src_mask=src_mask.cuda())\n",
    "\n",
    "                # Compute loss\n",
    "                loss = self.criterion(outputs.type(torch.float32), targets.type(torch.float32))\n",
    "                valid_loss += loss.item()\n",
    "\n",
    "        # Calculate average validation loss\n",
    "        valid_loss /= len(valid_loader.dataset)\n",
    "        return valid_loss\n",
    "\n",
    "        \n",
    "    def train_model(self, train_loader, valid_loader) -> None:\n",
    "        \"\"\"\n",
    "        Trains the model over multiple epochs, with early stopping and TensorBoard logging.\n",
    "\n",
    "        Args:\n",
    "            train_loader (DataLoader): The DataLoader for the training dataset.\n",
    "            valid_loader (DataLoader): The DataLoader for the validation dataset.\n",
    "        \"\"\"\n",
    "        self.model.cuda()  # Move model to GPU\n",
    "        best_valid_loss = float('inf')\n",
    "        early_stopping_counter = 0\n",
    "\n",
    "        # Set up TensorBoard writers for logging training and validation losses\n",
    "        log_dir = f'./data/Results/{self.model_type}/{self._name}/logs'\n",
    "        weights_dir = f'./data/Weights/{self.model_type}/{self._name}/'\n",
    "        train_writer = SummaryWriter(log_dir= log_dir + '/Train')\n",
    "        valid_writer = SummaryWriter(log_dir= log_dir + f'/Valid')\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            # Train for one epoch\n",
    "            train_loss = self.train(train_loader, epoch)\n",
    "            torch.save(self.model.state_dict(), weights_dir + 'current/checkpoint.pth')\n",
    "\n",
    "            # Evaluate on the validation set\n",
    "            valid_loss = self.eval(valid_loader)\n",
    "\n",
    "            # Log losses\n",
    "            train_loss /= len(train_loader.dataset)\n",
    "            valid_loss /= len(valid_loader.dataset)\n",
    "            print(f\"Epoch {epoch + 1}/{self.epochs}, Train Loss: {train_loss:.4f}, Valid Loss: {valid_loss:.4f}\")\n",
    "\n",
    "            train_writer.add_scalar('Loss', train_loss, epoch)\n",
    "            valid_writer.add_scalar('Loss', valid_loss, epoch)\n",
    "\n",
    "            # Save the model checkpoint if validation loss improves\n",
    "            if valid_loss < best_valid_loss:\n",
    "                best_valid_loss = valid_loss\n",
    "                torch.save(self.model.state_dict(), weights_dir + '/best_chkp/checkpoint.pth')\n",
    "                print(\"Checkpoint saved.\")\n",
    "                early_stopping_counter = 0\n",
    "            else:\n",
    "                early_stopping_counter += 1\n",
    "\n",
    "            # Early stopping if validation loss does not improve for a number of epochs\n",
    "            if early_stopping_counter >= self.early_stopping_patience:\n",
    "                print(f\"Validation loss hasn't decreased for {self.early_stopping_patience} epochs. Stopping training.\")\n",
    "                break\n",
    "\n",
    "        # Close TensorBoard writers\n",
    "        train_writer.close()\n",
    "        valid_writer.close()\n",
    "\n",
    "\n",
    "    def calculate_ADE(self, predicted_trajectories, true_trajectories):\n",
    "        \"\"\"\n",
    "        Calculates the Average Displacement Error (ADE) between predicted and true trajectories.\n",
    "\n",
    "        Args:\n",
    "            predicted_trajectories (np.array): The predicted trajectory points.\n",
    "            true_trajectories (np.array): The true trajectory points.\n",
    "\n",
    "        Returns:\n",
    "            float: The Average Displacement Error (ADE).\n",
    "        \"\"\"\n",
    "        errors = np.linalg.norm(predicted_trajectories - true_trajectories, axis=-1)\n",
    "        ADE = np.mean(errors, axis=-1)\n",
    "        return ADE\n",
    "\n",
    "\n",
    "    def calculate_FDE(self, predicted_trajectories, true_trajectories):\n",
    "        \"\"\"\n",
    "        Calculates the Final Displacement Error (FDE) between predicted and true trajectories.\n",
    "\n",
    "        Args:\n",
    "            predicted_trajectories (np.array): The predicted trajectory points.\n",
    "            true_trajectories (np.array): The true trajectory points.\n",
    "\n",
    "        Returns:\n",
    "            np.array: The Final Displacement Error (FDE).\n",
    "        \"\"\"\n",
    "        errors = np.linalg.norm(predicted_trajectories[:, -1] - true_trajectories[:, -1], axis=-1)\n",
    "        return errors\n",
    "\n",
    "\n",
    "    def calculate_FDE(self, predicted_trajectories, true_trajectories):\n",
    "        \"\"\"\n",
    "        Calculates the Final Displacement Error (FDE) between predicted and true trajectories.\n",
    "        The FDE measures the Euclidean distance between the predicted and actual positions \n",
    "        at the final time step of a trajectory.\n",
    "\n",
    "        Args:\n",
    "            predicted_trajectories (np.array): The predicted trajectory points.\n",
    "            true_trajectories (np.array): The true trajectory points.\n",
    "\n",
    "        Returns:\n",
    "            np.array: The Final Displacement Error (FDE) for each prediction.\n",
    "        \"\"\"\n",
    "        # Calculate the Euclidean distance between predicted and true trajectories at the final timestep\n",
    "        errors = np.linalg.norm(predicted_trajectories[:, -1] - true_trajectories[:, -1], axis=-1)\n",
    "        return errors\n",
    "\n",
    "\n",
    "    def validation_metrics(self, predicted_trajectories, true_trajectories):\n",
    "        \"\"\"\n",
    "        Computes validation metrics, including the Average Displacement Error (ADE) and \n",
    "        Final Displacement Error (FDE), and identifies the minimum ADE and FDE.\n",
    "\n",
    "        Args:\n",
    "            predicted_trajectories (np.array): The predicted trajectory points.\n",
    "            true_trajectories (np.array): The true trajectory points.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing the following metrics:\n",
    "                - ADE (np.array): The Average Displacement Error for each prediction.\n",
    "                - FDE (np.array): The Final Displacement Error for each prediction.\n",
    "                - minADE (float): The minimum value of the ADE.\n",
    "                - minFDE (float): The minimum value of the FDE.\n",
    "        \"\"\"\n",
    "        # Calculate ADE and FDE using the helper functions\n",
    "        ADE = self.calculate_ADE(predicted_trajectories, true_trajectories)\n",
    "        FDE = self.calculate_FDE(predicted_trajectories, true_trajectories)\n",
    "        \n",
    "        # Find indices of the minimum values for ADE and FDE\n",
    "        minADE_idx = np.argmin(ADE)\n",
    "        minFDE_idx = np.argmin(FDE)\n",
    "        \n",
    "        # Get the minimum ADE and FDE values\n",
    "        minADE = ADE[minADE_idx]\n",
    "        minFDE = FDE[minFDE_idx]\n",
    "        \n",
    "        return ADE, FDE, minADE, minFDE\n",
    "\n",
    "\n",
    "    def test_model(self, test_loader):\n",
    "        \"\"\"\n",
    "        Evaluates the trained model on the test dataset, calculating prediction accuracy\n",
    "        using ADE and FDE. Logs results to TensorBoard and saves the predictions and \n",
    "        true values to disk for later analysis.\n",
    "\n",
    "        Args:\n",
    "            test_loader (DataLoader): The DataLoader for the test dataset.\n",
    "        \"\"\"\n",
    "        # Set model to evaluation mode and move it to GPU\n",
    "        self.model.cuda()\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Initialize lists to store results and predictions\n",
    "        all_ADE = np.array([])\n",
    "        all_FDE = np.array([])\n",
    "        all_predictions = np.array([])\n",
    "        all_actuals = np.array([])\n",
    "\n",
    "        # Set up TensorBoard writer for logging\n",
    "        writer = SummaryWriter()\n",
    "\n",
    "        # Evaluate the model on the test dataset without gradient computation\n",
    "        with torch.no_grad():\n",
    "            src_tensors = np.array([])\n",
    "            tgt_tensor = np.array([])\n",
    "            pred_tensor = np.array([])\n",
    "\n",
    "            # Iterate over the test data batches\n",
    "            for batch_idx, batch in enumerate(test_loader):\n",
    "                # Retrieve inputs and targets for each model type\n",
    "                if self.model_type == 'Base':\n",
    "                    src, tgt = batch['src'].cuda(), batch['tgt'].cuda()\n",
    "                    src_mask = self.create_mask(src.shape[0], src.shape[1])\n",
    "                    tgt_mask = self.create_mask(tgt.shape[0], tgt.shape[1])\n",
    "                    inputs = self.scaler.scale(src[:, :, :3], \"src\")\n",
    "                    targets = self.scaler.scale(tgt[:, :, :2], \"tgt\")\n",
    "                    outputs = self.model(inputs[:, :, :2], targets, src_mask=src_mask.cuda(), tgt_mask=tgt_mask.cuda())\n",
    "\n",
    "                elif self.model_type == 'Transformer':\n",
    "                    src, tgt = batch['src'].cuda(), batch['tgt'].cuda()\n",
    "                    src_mask = self.create_mask(src.shape[0], src.shape[1])\n",
    "                    tgt_mask = self.create_mask(tgt.shape[0], tgt.shape[1])\n",
    "                    scaled_inputs = self.scaler.scale(src[:, :, :3], \"src\")\n",
    "                    targets = self.scaler.scale(tgt[:, :, :2], \"tgt\")\n",
    "                    inputs = torch.cat((scaled_inputs, src[:, :, 4:].cuda()), dim=2)\n",
    "                    outputs = self.model(inputs, targets, src_mask=src_mask.cuda(), tgt_mask=tgt_mask.cuda())\n",
    "\n",
    "                elif self.model_type == 'STAR' or self.model_type == 'SAESTAR':\n",
    "                    src, tgt, distances, distance_types = batch['src'].cuda(), batch['tgt'].cuda(), batch['distance'].cuda(), batch['type'].cuda()\n",
    "                    src_mask = self.create_mask(src.shape[0], src.shape[1])\n",
    "                    scaled_inputs = self.scaler.scale(src[:, :, :3], \"src\")\n",
    "                    targets = self.scaler.scale(tgt[:, :, :2], \"tgt\")\n",
    "                    distances = self.scaler.scale(distances, \"dist\")\n",
    "                    inputs = torch.cat((scaled_inputs, src[:, :, 4:].cuda()), dim=2)\n",
    "                    outputs = self.model(inputs.type(torch.float32), distances.type(torch.float32), distance_types, src_mask=src_mask.cuda())\n",
    "\n",
    "                # Unscale the model outputs to original values\n",
    "                new_outputs = self.scaler.unscale(outputs, \"tgt\", tgt[:, :, :2].shape)\n",
    "                \n",
    "                # Calculate validation metrics (ADE, FDE)\n",
    "                ADE, FDE, minADE, minFDE = self.validation_metrics(new_outputs.cpu().numpy(), tgt[:, :, :2].cpu().numpy())\n",
    "                \n",
    "                if pred_tensor.shape[0] == 0:\n",
    "                    # Collect predictions, targets, and sources for later processing\n",
    "                    pred_tensor =  new_outputs.cpu().numpy()\n",
    "                    tgt_tensor = tgt.cpu().numpy()\n",
    "                    src_tensors = src.cpu().numpy()\n",
    "                                        \n",
    "                    all_ADE = ADE\n",
    "                    all_FDE = FDE\n",
    "\n",
    "                    # Save predictions and true values\n",
    "                    all_predictions = new_outputs.cpu().numpy()\n",
    "                    all_actuals = targets[:, :, :2].cpu().numpy()\n",
    "                \n",
    "                # Collect predictions, targets, and sources for later processing\n",
    "                pred_tensor = np.append(pred_tensor, new_outputs.cpu().numpy(), axis=0)\n",
    "                tgt_tensor = np.append(tgt_tensor, tgt.cpu().numpy(), axis=0)\n",
    "                src_tensors = np.append(src_tensors, src.cpu().numpy(), axis=0)\n",
    "                \n",
    "                all_ADE = np.append(all_ADE, ADE, axis=0)\n",
    "                all_FDE = np.append(all_FDE, FDE, axis=0)\n",
    "\n",
    "                # Save predictions and true values\n",
    "                all_predictions = np.append(all_predictions, new_outputs.cpu().numpy(), axis=0)\n",
    "                all_actuals = np.append(all_actuals, targets[:, :, :2].cpu().numpy(), axis=0)\n",
    "                \n",
    "                # Log results to TensorBoard\n",
    "                writer.add_scalar('ADE/mean', ADE.mean())\n",
    "                writer.add_scalar('FDE/mean', FDE.mean())\n",
    "                writer.add_scalar('ADE/Min_mean', minADE.mean())\n",
    "                writer.add_scalar('FDE/Min_mean', minFDE.mean())\n",
    "\n",
    "            # Close the TensorBoard writer\n",
    "            writer.close()\n",
    "            \n",
    "            # Save the results to CSV\n",
    "            file_path = f\"./data/Results/{self.model_type}/{self._name}/src_pred_tgt_results.csv\"\n",
    "            with open(file_path, mode='w', newline='') as file:\n",
    "                csv_writer = csv.writer(file)\n",
    "                csv_writer.writerow([\"Source\", \"Prediction\", \"Target\"])\n",
    "\n",
    "                for i in range(src_tensors.shape[0]):\n",
    "                    for j in range(src_tensors.shape[1]):\n",
    "                        src_values = src_tensors[i, j, :]\n",
    "                        prediction_values = pred_tensor[i, j, :]\n",
    "                        target_values = tgt_tensor[i, j, :]\n",
    "                        csv_writer.writerow([src_values, prediction_values, target_values])\n",
    "\n",
    "        # Save tensors for later analysis\n",
    "        src_tensors = torch.tensor(src_tensors)\n",
    "        tgt_tensor = torch.tensor(tgt_tensor)\n",
    "        pred_tensor = torch.tensor(pred_tensor)\n",
    "\n",
    "        torch.save(src_tensors, f\"./data/Results/{self.model_type}/{self._name}/src.pt\")\n",
    "        torch.save(tgt_tensor, f\"./data/Results/{self.model_type}/{self._name}/tgt.pt\")\n",
    "        torch.save(pred_tensor, f\"./data/Results/{self.model_type}/{self._name}/pred.pt\")\n",
    "\n",
    "        # Calculate overall metrics across all batches\n",
    "        mean_ADE = np.mean(all_ADE)\n",
    "        mean_FDE = np.mean(all_FDE)\n",
    "\n",
    "        # Find the minimum ADE and FDE values\n",
    "        min_ADE_idx = np.argmin(all_ADE)\n",
    "        min_FDE_idx = np.argmin(all_FDE)\n",
    "        min_ADE = all_ADE[min_ADE_idx]\n",
    "        min_FDE = all_FDE[min_FDE_idx]\n",
    "\n",
    "        # Save the overall metrics to a file\n",
    "        save_file = f\"./data/Results/{self.model_type}/{self._name}/metrics.txt\"\n",
    "        with open(save_file, \"w\") as file:\n",
    "            file.write(f\"Mean ADE: {mean_ADE}\\n\")\n",
    "            file.write(f\"Mean FDE: {mean_FDE}\\n\")\n",
    "            file.write(f\"Min ADE: {min_ADE}\\n\")\n",
    "            file.write(f\"Min FDE: {min_FDE}\\n\")\n",
    "\n",
    "        print(\"Validation results saved to\", save_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperimentManager:\n",
    "    \"\"\"\n",
    "    Manages the lifecycle of experiments, including data preparation, model training, and evaluation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        location_name: str, \n",
    "        epochs: int, \n",
    "        learning_rate: float, \n",
    "        num_layers: int, \n",
    "        num_heads: int, \n",
    "        dropout: float, \n",
    "        src_len: int, \n",
    "        tgt_len: int, \n",
    "        batch_size: int,\n",
    "        hidden_size: int,\n",
    "        earlystopping : int\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the ExperimentManager with necessary configurations and perform initial visualizations.\n",
    "        \n",
    "        Parameters:\n",
    "        - location: Location object with dataset details.\n",
    "        - visualization: Visualization object for data insights.\n",
    "        - epochs: Number of training epochs.\n",
    "        - learning_rate: Learning rate for optimization.\n",
    "        - num_layers: Number of layers in the model.\n",
    "        - num_heads: Number of attention heads in the model.\n",
    "        - dropout: Dropout rate for regularization.\n",
    "        - src_len: Source sequence length for the model.\n",
    "        - tgt_len: Target sequence length for the model.\n",
    "        - batch_size: Batch size for training.\n",
    "        - hidden_size: Hidden layer size in the model.\n",
    "        \"\"\"\n",
    "        self.location_name = location_name\n",
    "        self.epochs = epochs\n",
    "        self.lr = learning_rate\n",
    "        self.layers = num_layers\n",
    "        self.heads = num_heads\n",
    "        self.dropout = dropout\n",
    "        self.batch_size = batch_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.earlystopping = earlystopping\n",
    "        self.src_len = src_len\n",
    "        self.tgt_len = tgt_len\n",
    "        \n",
    "    \n",
    "    def load_tensors(self, model_name : str, data_type : str, spatial : bool=False):\n",
    "        \"\"\"\n",
    "        Loads tensors from disk for a given model and data type, optionally including spatial data.\n",
    "\n",
    "        Args:\n",
    "            model_name (str): The name of the model (used for logging).\n",
    "            data_type (str): The type of data to load (e.g., 'train', 'validation', 'test').\n",
    "            spatial (bool): Whether to include spatial data ('dist' and 'dist_type') in the returned tensors.\n",
    "        \n",
    "        Returns:\n",
    "            dict: A dictionary containing loaded tensors, with keys 'src', 'tgt', and optionally 'distance', 'type'.\n",
    "        \"\"\"\n",
    "        print(f\"Loading {data_type} for model {model_name} at Location: {self.location_name}\")\n",
    "        \n",
    "        # Loading mandatory tensors\n",
    "        src = torch.load(f\"./data/Dataset/{self.location_name}/{data_type}/src.pt\", weights_only=True)\n",
    "        tgt = torch.load(f\"./data/Dataset/{self.location_name}/{data_type}/tgt.pt\", weights_only=True)\n",
    "        \n",
    "        if spatial:\n",
    "            # Loading additional spatial data if requested\n",
    "            dist = torch.load(f\"./data/Dataset/{self.location_name}/{data_type}/dist.pt\", weights_only=True)\n",
    "            d_type = torch.load(f\"./data/Dataset/{self.location_name}/{data_type}/dist_type.pt\", weights_only=True)\n",
    "            return {'src': src, 'tgt': tgt, 'distance': dist, 'type': d_type}\n",
    "        else:\n",
    "            return {'src': src, 'tgt': tgt}\n",
    "    \n",
    "    def experiment_base(self):\n",
    "        \"\"\"\n",
    "        Runs the base experiment, handling data preparation and experiment execution.\n",
    "        \"\"\"\n",
    "        model_name = 'Base'\n",
    "        print(\"Base Experiment Starting\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        # Loading and preparing data\n",
    "        train = self.load_tensors(model_name=model_name, data_type=\"Train\", spatial=False)\n",
    "        val = self.load_tensors(model_name=model_name, data_type=\"Val\", spatial=False)\n",
    "        test = self.load_tensors(model_name=model_name, data_type=\"Test\", spatial=False)\n",
    "        train_dataset = TransformerDataset(train)\n",
    "        val_dataset = TransformerDataset(val)\n",
    "        test_dataset = TransformerDataset(test)\n",
    "        \n",
    "        # Scaling data and creating DataLoaders\n",
    "        scaler = Scaler(train, False)\n",
    "        train_dataloader, val_dataloader, test_dataloader = self.get_data_loaders(train_dataset, val_dataset, test_dataset)\n",
    "        \n",
    "        # Running the experiment\n",
    "        self.run_experiment(model_name, scaler, train_dataloader, val_dataloader, test_dataloader, num_types=None, graph_dims=None)\n",
    "        \n",
    "        # Cleaning up memory\n",
    "        del scaler, train, val, test, train_dataloader, test_dataloader, val_dataloader\n",
    "        gc.collect()\n",
    "    \n",
    "    def experiment_transformer(self):\n",
    "        \"\"\"\n",
    "        Runs the Transformer experiment, handling data preparation and experiment execution.\n",
    "        \"\"\"\n",
    "        model_name = 'Transformer'\n",
    "        print(\"Transformer Experiment Starting\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        # Loading and preparing data\n",
    "        train = self.load_tensors(model_name=model_name, data_type=\"Train\", spatial=False)\n",
    "        val = self.load_tensors(model_name=model_name, data_type=\"Val\", spatial=False)\n",
    "        test = self.load_tensors(model_name=model_name, data_type=\"Test\", spatial=False)\n",
    "        train_dataset = TransformerDataset(train)\n",
    "        val_dataset = TransformerDataset(val)\n",
    "        test_dataset = TransformerDataset(test)\n",
    "        \n",
    "        # Scaling data and creating DataLoaders\n",
    "        scaler = Scaler(train, False)\n",
    "        train_dataloader, val_dataloader, test_dataloader = self.get_data_loaders(train_dataset, val_dataset, test_dataset)\n",
    "        \n",
    "        # Running the experiment\n",
    "        self.run_experiment(model_name, scaler, train_dataloader, val_dataloader, test_dataloader, num_types=None, graph_dims=None)\n",
    "        \n",
    "        # Cleaning up memory\n",
    "        del scaler, train, val, test, train_dataloader, test_dataloader, val_dataloader\n",
    "        gc.collect()\n",
    "    \n",
    "    def experiment_star(self):\n",
    "        \"\"\"\n",
    "        Runs the STAR experiment, handling data preparation and experiment execution.\n",
    "        \"\"\"\n",
    "        model_name = 'STAR'\n",
    "        print(\"STAR Experiment Starting\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        # Loading and preparing data\n",
    "        train = self.load_tensors(model_name=model_name, data_type=\"Train\", spatial=True)\n",
    "        val = self.load_tensors(model_name=model_name, data_type=\"Val\", spatial=True)\n",
    "        test = self.load_tensors(model_name=model_name, data_type=\"Test\", spatial=True)\n",
    "        graph_dims = len(train['distance'][0, 0, :self.num_agents])\n",
    "        train_dataset = STARDataset(train, self.num_agents)\n",
    "        val_dataset = STARDataset(val, self.num_agents)\n",
    "        test_dataset = STARDataset(test, self.num_agents)\n",
    "        \n",
    "        # Scaling data and creating DataLoaders\n",
    "        scaler = Scaler(train, model_name, True, graph_dims)\n",
    "        train_dataloader, val_dataloader, test_dataloader = self.get_data_loaders(train_dataset, val_dataset, test_dataset)\n",
    "        \n",
    "        # Running the experiment\n",
    "        self.run_experiment(model_name, scaler, train_dataloader, val_dataloader, test_dataloader, num_types=None, graph_dims=graph_dims)\n",
    "        \n",
    "        # Cleaning up memory\n",
    "        del scaler, train, val, test, train_dataloader, test_dataloader, val_dataloader\n",
    "        gc.collect()\n",
    "    \n",
    "    def experiment_saestar(self):\n",
    "        \"\"\"\n",
    "        Runs the SAESTAR experiment, handling data preparation and experiment execution.\n",
    "        \"\"\"\n",
    "        model_name = 'SAESTAR'\n",
    "        print(\"SAESTAR Experiment Starting\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        # Loading and preparing data\n",
    "        train = self.load_tensors(model_name=model_name, data_type=\"Train\", spatial=True)\n",
    "        val = self.load_tensors(model_name=model_name, data_type=\"Val\", spatial=True)\n",
    "        test = self.load_tensors(model_name=model_name, data_type=\"Test\", spatial=True)\n",
    "        df_env = pd.read_csv(f\"./data/CombinedData/{self.location_name}/env_df.csv\", sep=',')\n",
    "        num_types = 4 + len(df_env['ID'].unique())\n",
    "        graph_dims = len(train['distance'][0, 0, :])\n",
    "        train_dataset = SAESTARDataset(train)\n",
    "        val_dataset = SAESTARDataset(val)\n",
    "        test_dataset = SAESTARDataset(test)\n",
    "        \n",
    "        # Scaling data and creating DataLoaders\n",
    "        scaler = Scaler(train, model_name, True)\n",
    "        train_dataloader, val_dataloader, test_dataloader = self.get_data_loaders(train_dataset, val_dataset, test_dataset)\n",
    "        \n",
    "        # Running the experiment\n",
    "        self.run_experiment(model_name, scaler, train_dataloader, val_dataloader, test_dataloader, num_types=num_types, graph_dims=graph_dims)\n",
    "        \n",
    "        # Cleaning up memory\n",
    "        del scaler, train, val, test, train_dataloader, test_dataloader, val_dataloader\n",
    "        gc.collect()\n",
    "    \n",
    "    def run_experiment(self, model_name, scaler, train, val, test, num_types, graph_dims):\n",
    "        \"\"\"\n",
    "        Executes the training and testing of the specified model.\n",
    "        \n",
    "        Args:\n",
    "            model_name (str): The name of the model being tested.\n",
    "            scaler (Scaler): An object to scale and normalize the data.\n",
    "            train, val, test (DataLoader): DataLoaders for the respective datasets.\n",
    "            num_types (int): The number of types or classes (if applicable).\n",
    "            graph_dims (int): The dimensionality of graph data (if applicable).\n",
    "        \"\"\"\n",
    "        print(f\"{model_name} Experiment Initiating\")\n",
    "        \n",
    "        # Initializing the experiment with the provided configuration\n",
    "        experiment = Experiment(\n",
    "            scaler=scaler,\n",
    "            model_name=model_name,\n",
    "            src_len=self.src_len,\n",
    "            tgt_len=self.tgt_len,\n",
    "            num_types=num_types,\n",
    "            graph_dims=graph_dims,\n",
    "            layers=self.layers,\n",
    "            heads=self.heads,\n",
    "            hidden_size=self.hidden_size,\n",
    "            dropout=self.dropout,\n",
    "            lr=self.lr,\n",
    "            epochs=self.epochs,\n",
    "            location_name=self.location_name,\n",
    "            earlystopping=self.earlystopping\n",
    "        )\n",
    "        \n",
    "        # Training and testing the model\n",
    "        experiment.train_model(train, val)\n",
    "        experiment.test_model(test)\n",
    "        \n",
    "        # Cleaning up memory\n",
    "        del experiment\n",
    "        gc.collect()\n",
    "    \n",
    "    def get_data_loaders(self, train : Dataset, val : Dataset, test : Dataset):\n",
    "        \"\"\"\n",
    "        Creates DataLoader objects for training, validation, and testing datasets.\n",
    "        \n",
    "        Args:\n",
    "            train, val, test (Dataset): Datasets for training, validation, and testing.\n",
    "        \n",
    "        Returns:\n",
    "            tuple[DataLoader]: DataLoader objects for training, validation, and testing.\n",
    "        \"\"\"\n",
    "        train_dataloader = DataLoader(train, batch_size=self.batch_size, shuffle=True)\n",
    "        val_dataloader = DataLoader(val, batch_size=self.batch_size, shuffle=True)\n",
    "        test_dataloader = DataLoader(test, batch_size=self.batch_size, shuffle=False)\n",
    "        \n",
    "        return train_dataloader, val_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available \n",
    "gpu = torch.cuda.is_available()\n",
    "print(\"GPU Available: \", gpu)\n",
    "# Apply deterministic on CUDA convoltion operations\n",
    "torch.backends.cudnn.deterministic = True\n",
    "# Disable benchmark mode\n",
    "torch.backends.cudnn.benchmark = False\n",
    "# Create a manual seed for testing\n",
    "torch.manual_seed(42)\n",
    "\n",
    "epochs = 1\n",
    "num_layers = 16\n",
    "num_heads = 8\n",
    "dropout = 0.3\n",
    "learning_rate = 0.000015\n",
    "src_len = 10\n",
    "tgt_len = 40\n",
    "batch_size = 32\n",
    "hidden_size = 512\n",
    "earlystopping = 30\n",
    "\n",
    "\n",
    "experiment_manager = ExperimentManager(\n",
    "        location_name=\"Torpagatan\", \n",
    "        epochs=epochs, \n",
    "        learning_rate=learning_rate, \n",
    "        num_layers=num_layers, \n",
    "        num_heads=num_heads, \n",
    "        dropout=dropout,\n",
    "        src_len=src_len,\n",
    "        tgt_len=tgt_len,\n",
    "        batch_size=batch_size,\n",
    "        hidden_size=hidden_size,\n",
    "        earlystopping=earlystopping\n",
    "        )\n",
    "    \n",
    "experiment_manager.experiment_base()\n",
    "experiment_manager.experiment_transformer()\n",
    "experiment_manager.experiment_star()\n",
    "experiment_manager.experiment_saestar()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
